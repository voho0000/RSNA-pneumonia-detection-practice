{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/sovit-123/Pneumonia-Detection-using-Deep-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T00:15:57.385145Z",
     "start_time": "2020-12-05T00:15:57.380513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "\"\"\"\n",
    "Python script to prepare FasterRCNN model.\n",
    "\"\"\"\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "def model():\n",
    "    # load a pre-trained model for classification and return only the features\n",
    "    backbone = torchvision.models.densenet121(pretrained=True).features\n",
    "\n",
    "    # FasterRCNN needs to know the number of output channels in a backbone. \n",
    "    # For densenet121, it's 1024\n",
    "    backbone.out_channels = 1024\n",
    "\n",
    "    # let's make the RPN generate 5 x 3 anchors per spatial location\n",
    "    # with 5 different sizes and 3 different aspect ratios. \n",
    "    # We have a Tuple[Tuple[int]] because each feature map could potentially have different sizes and aspect ratios\n",
    "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                       aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "    # let's define what are the feature maps that we will use to perform the region of interest cropping,\n",
    "    # as well as the size of the crop after rescaling.\n",
    "    # if your backbone returns a Tensor, featmap_names is expected to be [0]. \n",
    "    # More generally, the backbone should return an OrderedDict[Tensor], \n",
    "    # and in featmap_names you can choose which feature maps to use.\n",
    "\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                    output_size=7,\n",
    "                                                    sampling_ratio=2)\n",
    "\n",
    "    # put the pieces together inside a FasterRCNN model\n",
    "    model = FasterRCNN(backbone,\n",
    "                       num_classes=2,\n",
    "                       rpn_anchor_generator=anchor_generator,\n",
    "                       box_roi_pool=roi_pooler)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T00:15:57.398510Z",
     "start_time": "2020-12-05T00:15:57.386948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset.py\n",
    "\n",
    "\"\"\"\n",
    "Python script to prepare the dataset\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import re\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RSNADataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_ids = dataframe['patientId'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        image_id = self.image_ids[index]\n",
    "        records = self.df[self.df['patientId'] == image_id]\n",
    "\n",
    "        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        boxes = records[['x', 'y', 'width', 'height']].values\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        # target['masks'] = None\n",
    "        target['patientId'] = torch.tensor([index])\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = iscrowd\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "            target['boxes'] = torch.stack(tuple(map(torch.FloatTensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T00:15:57.405945Z",
     "start_time": "2020-12-05T00:15:57.399564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile engine.py\n",
    "\n",
    "import pandas as pd\n",
    "import dataset\n",
    "import albumentations as A\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "from albumentations import (\n",
    "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Complete mAP code here => https://gist.github.com/tarlen5/008809c3decf19313de216b9208f3734\n",
    "\"\"\"\n",
    "\n",
    "def calculate_image_precision(gts, preds, thresholds = (0.5, ), form = 'coco') -> float:\n",
    "    # https://www.kaggle.com/sadmanaraf/wheat-detection-using-faster-rcnn-train\n",
    "    \"\"\"Calculates image precision.\n",
    "\n",
    "    Args:\n",
    "        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n",
    "        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n",
    "               sorted by confidence value (descending)\n",
    "        thresholds: (float) Different thresholds\n",
    "        form: (str) Format of the coordinates\n",
    "\n",
    "    Return:\n",
    "        (float) Precision\n",
    "    \"\"\"\n",
    "    n_threshold = len(thresholds)\n",
    "    image_precision = 0.0\n",
    "    \n",
    "    ious = np.ones((len(gts), len(preds))) * -1\n",
    "    # ious = None\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        precision_at_threshold = calculate_precision(gts.copy(), preds, threshold=threshold,\n",
    "                                                     form=form, ious=ious)\n",
    "        image_precision += precision_at_threshold / n_threshold\n",
    "\n",
    "    return image_precision\n",
    "\n",
    "\n",
    "def calculate_iou(gt, pr, form='pascal_voc') -> float:\n",
    "    # https://www.kaggle.com/sadmanaraf/wheat-detection-using-faster-rcnn-train\n",
    "    \"\"\"Calculates the Intersection over Union.\n",
    "\n",
    "    Args:\n",
    "        gt: (np.ndarray[Union[int, float]]) coordinates of the ground-truth box\n",
    "        pr: (np.ndarray[Union[int, float]]) coordinates of the prdected box\n",
    "        form: (str) gt/pred coordinates format\n",
    "            - pascal_voc: [xmin, ymin, xmax, ymax]\n",
    "            - coco: [xmin, ymin, w, h]\n",
    "    Returns:\n",
    "        (float) Intersection over union (0.0 <= iou <= 1.0)\n",
    "    \"\"\"\n",
    "    if form == 'coco':\n",
    "        gt = gt.copy()\n",
    "        pr = pr.copy()\n",
    "\n",
    "        gt[2] = gt[0] + gt[2]\n",
    "        gt[3] = gt[1] + gt[3]\n",
    "        pr[2] = pr[0] + pr[2]\n",
    "        pr[3] = pr[1] + pr[3]\n",
    "\n",
    "    # Calculate overlap area\n",
    "    dx = min(gt[2], pr[2]) - max(gt[0], pr[0]) + 1\n",
    "    \n",
    "    if dx < 0:\n",
    "        return 0.0\n",
    "    dy = min(gt[3], pr[3]) - max(gt[1], pr[1]) + 1\n",
    "\n",
    "    if dy < 0:\n",
    "        return 0.0\n",
    "\n",
    "    overlap_area = dx * dy\n",
    "\n",
    "    # Calculate union area\n",
    "    union_area = (\n",
    "            (gt[2] - gt[0] + 1) * (gt[3] - gt[1] + 1) +\n",
    "            (pr[2] - pr[0] + 1) * (pr[3] - pr[1] + 1) -\n",
    "            overlap_area\n",
    "    )\n",
    "\n",
    "    return overlap_area / union_area\n",
    "\n",
    "\n",
    "def find_best_match(gts, pred, pred_idx, threshold = 0.5, form = 'pascal_voc', ious=None) -> int:\n",
    "    # https://www.kaggle.com/sadmanaraf/wheat-detection-using-faster-rcnn-train\n",
    "    \"\"\"Returns the index of the 'best match' between the\n",
    "    ground-truth boxes and the prediction. The 'best match'\n",
    "    is the highest IoU. (0.0 IoUs are ignored).\n",
    "\n",
    "    Args:\n",
    "        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n",
    "        pred: (List[Union[int, float]]) Coordinates of the predicted box\n",
    "        pred_idx: (int) Index of the current predicted box\n",
    "        threshold: (float) Threshold\n",
    "        form: (str) Format of the coordinates\n",
    "        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n",
    "\n",
    "    Return:\n",
    "        (int) Index of the best match GT box (-1 if no match above threshold)\n",
    "    \"\"\"\n",
    "    best_match_iou = -np.inf\n",
    "    best_match_idx = -1\n",
    "    for gt_idx in range(len(gts)):\n",
    "        \n",
    "        if gts[gt_idx][0] < 0:\n",
    "            # Already matched GT-box\n",
    "            continue\n",
    "        \n",
    "        iou = -1 if ious is None else ious[gt_idx][pred_idx]\n",
    "\n",
    "        if iou < 0:\n",
    "            iou = calculate_iou(gts[gt_idx], pred, form=form)\n",
    "            \n",
    "            if ious is not None:\n",
    "                ious[gt_idx][pred_idx] = iou\n",
    "\n",
    "        if iou < threshold:\n",
    "            continue\n",
    "\n",
    "        if iou > best_match_iou:\n",
    "            best_match_iou = iou\n",
    "            best_match_idx = gt_idx\n",
    "\n",
    "    return best_match_idx\n",
    "\n",
    "def calculate_precision(gts, preds, threshold = 0.5, form = 'coco', ious=None) -> float:\n",
    "    # https://www.kaggle.com/sadmanaraf/wheat-detection-using-faster-rcnn-train\n",
    "    \"\"\"Calculates precision for GT - prediction pairs at one threshold.\n",
    "\n",
    "    Args:\n",
    "        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n",
    "        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n",
    "               sorted by confidence value (descending)\n",
    "        threshold: (float) Threshold\n",
    "        form: (str) Format of the coordinates\n",
    "        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n",
    "\n",
    "    Return:\n",
    "        (float) Precision\n",
    "    \"\"\"\n",
    "    n = len(preds)\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    for pred_idx in range(n):\n",
    "\n",
    "        best_match_gt_idx = find_best_match(gts, preds[pred_idx], pred_idx,\n",
    "                                            threshold=threshold, form=form, ious=ious)\n",
    "\n",
    "        if best_match_gt_idx >= 0:\n",
    "            # True positive: The predicted box matches a gt box with an IoU above the threshold.\n",
    "            tp += 1\n",
    "            # Remove the matched GT box\n",
    "            gts[best_match_gt_idx] = -1\n",
    "        else:\n",
    "            # No match\n",
    "            # False positive: indicates a predicted box had no associated gt box.\n",
    "            fp += 1\n",
    "\n",
    "    # False negative: indicates a gt box had no associated predicted box.\n",
    "    fn = (gts.sum(axis=1) > 0).sum()\n",
    "\n",
    "    return tp / (tp + fp + fn)\n",
    "\n",
    "\n",
    "# Albumentations\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Flip(0.5),\n",
    "        A.RandomRotate90(0.5),\n",
    "        MotionBlur(p=0.2),\n",
    "        MedianBlur(blur_limit=3, p=0.1),\n",
    "        Blur(blur_limit=3, p=0.1),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def prepare_data():\n",
    "    DIR_INPUT = '../data/size1024'\n",
    "    DIR_TRAIN = f\"{DIR_INPUT}/PA/\"\n",
    "\n",
    "    train_df = pd.read_csv(f\"{DIR_INPUT}/df_PA.csv\")\n",
    "    print(train_df.shape)\n",
    "    train_df.head()\n",
    "\n",
    "    train_df_pos = pd.DataFrame(columns=['patientId', 'x', 'y', 'width', 'height'])\n",
    "\n",
    "    k = 0\n",
    "    for i in range(len(train_df)):\n",
    "        if train_df.loc[i]['Target'] == 1:\n",
    "            train_df_pos.loc[k] = train_df.loc[i]\n",
    "            k += 1\n",
    "\n",
    "    image_ids = train_df_pos['patientId'].unique()\n",
    "    valid_ids = image_ids[-100:]\n",
    "    train_ids = image_ids[:-100]\n",
    "    print(f\"Training instance: {len(train_ids)}\")\n",
    "    print(f\"Validation instances: {len(valid_ids)}\")\n",
    "\n",
    "    valid_df = train_df_pos[train_df_pos['patientId'].isin(valid_ids)]\n",
    "    train_df = train_df_pos[train_df_pos['patientId'].isin(train_ids)]\n",
    "\n",
    "    valid_df.shape, train_df.shape\n",
    "    \n",
    "    train_dataset = dataset.RSNADataset(train_df, DIR_TRAIN, get_train_transform())\n",
    "    valid_dataset = dataset.RSNADataset(valid_df, DIR_TRAIN, get_valid_transform())\n",
    "    \n",
    "    return train_dataset, valid_dataset\n",
    "    \n",
    "def get_data_loader(batch_size):\n",
    "    \n",
    "    train_dataset, valid_dataset = prepare_data()\n",
    "    \n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4, # else showing broken pipe error\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4, # else showing broken pipe error\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    return train_data_loader, valid_data_loader\n",
    "\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "        \n",
    "def train(dataloader, lr_scheduler, model, optimizer, \n",
    "          device, epoch, loss_hist, itr):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    loss_hist.reset()\n",
    "    for images, targets, image_ids in dataloader:\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % 50 == 0:\n",
    "            print(f\"Epoch #{epoch} iteration #{itr} loss: {loss_value}\")\n",
    "\n",
    "        itr += 1\n",
    "    \n",
    "    end = time.time()\n",
    "    return loss_hist, end, start\n",
    "\n",
    "def validate(dataloader, model, device, iou_thresholds):\n",
    "    valid_image_precision = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, targets, image_ids in dataloader:\n",
    "\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            outputs = model(images)\n",
    "            \n",
    "    for i, image in enumerate(images):\n",
    "        boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
    "        scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "        gt_boxes = targets[i]['boxes'].cpu().numpy()\n",
    "        preds_sorted_idx = np.argsort(scores)[::-1]\n",
    "        preds_sorted = boxes[preds_sorted_idx]\n",
    "        image_precision = calculate_image_precision(preds_sorted,\n",
    "                                                        gt_boxes,\n",
    "                                                        thresholds=iou_thresholds,\n",
    "                                                        form='coco')\n",
    "        valid_image_precision.append(image_precision)\n",
    "\n",
    "    valid_prec = np.mean(valid_image_precision)\n",
    "    return valid_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-26T16:06:43.445278Z",
     "start_time": "2020-12-26T16:06:39.545314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14511, 23)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DIR_INPUT = '../data/size1024'\n",
    "DIR_TRAIN = f\"{DIR_INPUT}/PA/\"\n",
    "\n",
    "train_df = pd.read_csv(f\"{DIR_INPUT}/df_PA.csv\")\n",
    "print(train_df.shape)\n",
    "train_df.head()\n",
    "\n",
    "train_df_pos = pd.DataFrame(columns=['patientId', 'x', 'y', 'width', 'height'])\n",
    "\n",
    "k = 0\n",
    "for i in range(len(train_df)):\n",
    "    if train_df.loc[i]['Target'] == 1:\n",
    "        train_df_pos.loc[k] = train_df.loc[i]\n",
    "        k += 1\n",
    "\n",
    "image_ids = train_df_pos['patientId'].unique()\n",
    "valid_ids = image_ids[-100:]\n",
    "train_ids = image_ids[:-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-26T16:07:13.960745Z",
     "start_time": "2020-12-26T16:07:13.952282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'c44a8316-9b90-49e6-ab36-8db213312a4e' in valid_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T00:15:57.413705Z",
     "start_time": "2020-12-05T00:15:57.407141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import torch\n",
    "import engine\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "\n",
    "from engine import get_data_loader, Averager, train, validate\n",
    "from model import model\n",
    "# from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-s', '--show-sample', dest='show_sample', default='no', \n",
    "                 help='whether to visualize a wheat sample with bboxes or not')\n",
    "args = vars(parser.parse_args())\n",
    "\n",
    "# learning parameters\n",
    "num_epochs = 200\n",
    "lr = 0.001\n",
    "batch_size = 4\n",
    "\n",
    "model = model().to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=0.0005)\n",
    "# optimizer = torch.optim.Adam(params, lr=0.01)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "lr_scheduler = None\n",
    "\n",
    "# initialize the Averager\n",
    "loss_hist = engine.Averager()\n",
    "# get the dataloader\n",
    "train_data_loader, valid_data_loader = get_data_loader(batch_size)\n",
    "\n",
    "if args['show_sample'] == 'yes':\n",
    "    images, targets, image_ids = next(iter(train_data_loader))\n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    boxes = targets[2]['boxes'].cpu().numpy().astype(np.int32)\n",
    "    sample = images[2].permute(1,2,0).cpu().numpy()\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "    for box in boxes:\n",
    "        cv2.rectangle(sample,\n",
    "                      (box[0], box[1]),\n",
    "                      (box[2], box[3]),\n",
    "                      (220, 0, 0), 3)\n",
    "    \n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(sample)\n",
    "    plt.show()\n",
    "\n",
    "iou_thresholds = [x for x in np.arange(0.5, 0.76, 0.05)]\n",
    "\n",
    "train_loss = []\n",
    "precision = []\n",
    "for epoch in range(num_epochs):\n",
    "    itr = 1\n",
    "    train_loss_hist, end, start = train(train_data_loader, lr_scheduler,\n",
    "                                        model, optimizer, device,\n",
    "                                        epoch, loss_hist, itr)\n",
    "    valid_prec = validate(valid_data_loader, model, device, iou_thresholds)\n",
    "    print(f\"Took {(end-start)/60:.3f} minutes for epoch# {epoch} to train\")\n",
    "    print(f\"Epoch #{epoch} Train loss: {train_loss_hist.value}\")  \n",
    "    print(f\"Epoch #{epoch} Validation Precision: {valid_prec}\")  \n",
    "    train_loss.append(train_loss_hist.value)\n",
    "    precision.append(valid_prec)\n",
    "    \n",
    "    # update the learning rate\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "    if epoch % 50 == 0:\n",
    "        torch.save(model.state_dict(), f'fasterrcnn_densenet121_PA_fpn_{epoch}.pth')\n",
    "        \n",
    "torch.save(model.state_dict(), 'fasterrcnn_densenet121_PA_fpn_final.pth')\n",
    "\n",
    "# plot and save the training loss\n",
    "plt.figure()\n",
    "plt.plot(train_loss, label='Training loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('loss.png')\n",
    "\n",
    "# plot and save the validation precision\n",
    "plt.figure()\n",
    "plt.plot(precision, label='Validation precision')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('precision.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T06:24:35.739534Z",
     "start_time": "2020-12-05T00:15:57.414791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14511, 23)\n",
      "Training instance: 1248\n",
      "Validation instances: 100\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Figure(1600x800)\n",
      "/home/yihsin/anaconda3/lib/python3.7/site-packages/torchvision/ops/boxes.py:101: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  keep = keep.nonzero().squeeze(1)\n",
      "Epoch #0 iteration #50 loss: 0.19255131483078003\n",
      "Epoch #0 iteration #100 loss: 0.16032831370830536\n",
      "Epoch #0 iteration #150 loss: 0.15687504410743713\n",
      "Epoch #0 iteration #200 loss: 0.19096186757087708\n",
      "Epoch #0 iteration #250 loss: 0.16026833653450012\n",
      "Epoch #0 iteration #300 loss: 0.12801793217658997\n",
      "Took 1.819 minutes for epoch# 0 to train\n",
      "Epoch #0 Train loss: 0.23205801889968988\n",
      "Epoch #0 Validation Precision: 0.027109181141439207\n",
      "Epoch #1 iteration #50 loss: 0.1310964822769165\n",
      "Epoch #1 iteration #100 loss: 0.118324875831604\n",
      "Epoch #1 iteration #150 loss: 0.14714734256267548\n",
      "Epoch #1 iteration #200 loss: 0.2328725904226303\n",
      "Epoch #1 iteration #250 loss: 0.1839606761932373\n",
      "Epoch #1 iteration #300 loss: 0.15456622838974\n",
      "Took 1.761 minutes for epoch# 1 to train\n",
      "Epoch #1 Train loss: 0.16496585319057488\n",
      "Epoch #1 Validation Precision: 0.038527784851314265\n",
      "Epoch #2 iteration #50 loss: 0.1336984634399414\n",
      "Epoch #2 iteration #100 loss: 0.13551531732082367\n",
      "Epoch #2 iteration #150 loss: 0.1355074644088745\n",
      "Epoch #2 iteration #200 loss: 0.23445387184619904\n",
      "Epoch #2 iteration #250 loss: 0.19546586275100708\n",
      "Epoch #2 iteration #300 loss: 0.14819790422916412\n",
      "Took 1.758 minutes for epoch# 2 to train\n",
      "Epoch #2 Train loss: 0.15701225927720466\n",
      "Epoch #2 Validation Precision: 0.05309909388856757\n",
      "Epoch #3 iteration #50 loss: 0.14634276926517487\n",
      "Epoch #3 iteration #100 loss: 0.15306946635246277\n",
      "Epoch #3 iteration #150 loss: 0.1092146635055542\n",
      "Epoch #3 iteration #200 loss: 0.2546093165874481\n",
      "Epoch #3 iteration #250 loss: 0.17438222467899323\n",
      "Epoch #3 iteration #300 loss: 0.1740543693304062\n",
      "Took 1.763 minutes for epoch# 3 to train\n",
      "Epoch #3 Train loss: 0.15210476235892528\n",
      "Epoch #3 Validation Precision: 0.07451320609215346\n",
      "Epoch #4 iteration #50 loss: 0.12861526012420654\n",
      "Epoch #4 iteration #100 loss: 0.11426804214715958\n",
      "Epoch #4 iteration #150 loss: 0.11016463488340378\n",
      "Epoch #4 iteration #200 loss: 0.2208939641714096\n",
      "Epoch #4 iteration #250 loss: 0.13107025623321533\n",
      "Epoch #4 iteration #300 loss: 0.17625954747200012\n",
      "Took 1.757 minutes for epoch# 4 to train\n",
      "Epoch #4 Train loss: 0.14862411082364047\n",
      "Epoch #4 Validation Precision: 0.07684864953886693\n",
      "Epoch #5 iteration #50 loss: 0.1401498168706894\n",
      "Epoch #5 iteration #100 loss: 0.11135528981685638\n",
      "Epoch #5 iteration #150 loss: 0.09690585732460022\n",
      "Epoch #5 iteration #200 loss: 0.19863036274909973\n",
      "Epoch #5 iteration #250 loss: 0.17262987792491913\n",
      "Epoch #5 iteration #300 loss: 0.15942655503749847\n",
      "Took 1.758 minutes for epoch# 5 to train\n",
      "Epoch #5 Train loss: 0.14790849521374091\n",
      "Epoch #5 Validation Precision: 0.07300061050061049\n",
      "Epoch #6 iteration #50 loss: 0.1302383691072464\n",
      "Epoch #6 iteration #100 loss: 0.12408126890659332\n",
      "Epoch #6 iteration #150 loss: 0.11582674831151962\n",
      "Epoch #6 iteration #200 loss: 0.21159660816192627\n",
      "Epoch #6 iteration #250 loss: 0.1880331039428711\n",
      "Epoch #6 iteration #300 loss: 0.15757258236408234\n",
      "Took 1.760 minutes for epoch# 6 to train\n",
      "Epoch #6 Train loss: 0.14376543443172407\n",
      "Epoch #6 Validation Precision: 0.0808531746031746\n",
      "Epoch #7 iteration #50 loss: 0.128125861287117\n",
      "Epoch #7 iteration #100 loss: 0.10032393783330917\n",
      "Epoch #7 iteration #150 loss: 0.10397202521562576\n",
      "Epoch #7 iteration #200 loss: 0.17372314631938934\n",
      "Epoch #7 iteration #250 loss: 0.15504316985607147\n",
      "Epoch #7 iteration #300 loss: 0.15817415714263916\n",
      "Took 1.756 minutes for epoch# 7 to train\n",
      "Epoch #7 Train loss: 0.14158225556214651\n",
      "Epoch #7 Validation Precision: 0.09940476190476191\n",
      "Epoch #8 iteration #50 loss: 0.1408584713935852\n",
      "Epoch #8 iteration #100 loss: 0.10532994568347931\n",
      "Epoch #8 iteration #150 loss: 0.11234573274850845\n",
      "Epoch #8 iteration #200 loss: 0.21961677074432373\n",
      "Epoch #8 iteration #250 loss: 0.17381340265274048\n",
      "Epoch #8 iteration #300 loss: 0.15496572852134705\n",
      "Took 1.757 minutes for epoch# 8 to train\n",
      "Epoch #8 Train loss: 0.14003726484206241\n",
      "Epoch #8 Validation Precision: 0.11047008547008547\n",
      "Epoch #9 iteration #50 loss: 0.19324991106987\n",
      "Epoch #9 iteration #100 loss: 0.129740908741951\n",
      "Epoch #9 iteration #150 loss: 0.09334728121757507\n",
      "Epoch #9 iteration #200 loss: 0.18179334700107574\n",
      "Epoch #9 iteration #250 loss: 0.14519210159778595\n",
      "Epoch #9 iteration #300 loss: 0.16728657484054565\n",
      "Took 1.755 minutes for epoch# 9 to train\n",
      "Epoch #9 Train loss: 0.1421801283573493\n",
      "Epoch #9 Validation Precision: 0.08234126984126983\n",
      "Epoch #10 iteration #50 loss: 0.13803978264331818\n",
      "Epoch #10 iteration #100 loss: 0.10187574476003647\n",
      "Epoch #10 iteration #150 loss: 0.11848291754722595\n",
      "Epoch #10 iteration #200 loss: 0.15087997913360596\n",
      "Epoch #10 iteration #250 loss: 0.16454719007015228\n",
      "Epoch #10 iteration #300 loss: 0.14888343214988708\n",
      "Took 1.761 minutes for epoch# 10 to train\n",
      "Epoch #10 Train loss: 0.1385881234294711\n",
      "Epoch #10 Validation Precision: 0.07534722222222222\n",
      "Epoch #11 iteration #50 loss: 0.1300206482410431\n",
      "Epoch #11 iteration #100 loss: 0.09670094400644302\n",
      "Epoch #11 iteration #150 loss: 0.10708951950073242\n",
      "Epoch #11 iteration #200 loss: 0.15244369208812714\n",
      "Epoch #11 iteration #250 loss: 0.15689873695373535\n",
      "Epoch #11 iteration #300 loss: 0.1643417775630951\n",
      "Took 1.759 minutes for epoch# 11 to train\n",
      "Epoch #11 Train loss: 0.13670755428476974\n",
      "Epoch #11 Validation Precision: 0.08581349206349206\n",
      "Epoch #12 iteration #50 loss: 0.12506136298179626\n",
      "Epoch #12 iteration #100 loss: 0.11532553285360336\n",
      "Epoch #12 iteration #150 loss: 0.10522008687257767\n",
      "Epoch #12 iteration #200 loss: 0.19351227581501007\n",
      "Epoch #12 iteration #250 loss: 0.1399911642074585\n",
      "Epoch #12 iteration #300 loss: 0.15558719635009766\n",
      "Took 1.761 minutes for epoch# 12 to train\n",
      "Epoch #12 Train loss: 0.13649527633037323\n",
      "Epoch #12 Validation Precision: 0.10309829059829062\n",
      "Epoch #13 iteration #50 loss: 0.14560315012931824\n",
      "Epoch #13 iteration #100 loss: 0.13533452153205872\n",
      "Epoch #13 iteration #150 loss: 0.12288986146450043\n",
      "Epoch #13 iteration #200 loss: 0.12640710175037384\n",
      "Epoch #13 iteration #250 loss: 0.17169804871082306\n",
      "Epoch #13 iteration #300 loss: 0.13565999269485474\n",
      "Took 1.760 minutes for epoch# 13 to train\n",
      "Epoch #13 Train loss: 0.13463652523186725\n",
      "Epoch #13 Validation Precision: 0.10300925925925927\n",
      "Epoch #14 iteration #50 loss: 0.13200800120830536\n",
      "Epoch #14 iteration #100 loss: 0.08859237283468246\n",
      "Epoch #14 iteration #150 loss: 0.11773861199617386\n",
      "Epoch #14 iteration #200 loss: 0.1619529128074646\n",
      "Epoch #14 iteration #250 loss: 0.19000491499900818\n",
      "Epoch #14 iteration #300 loss: 0.15890851616859436\n",
      "Took 1.758 minutes for epoch# 14 to train\n",
      "Epoch #14 Train loss: 0.1337514177728922\n",
      "Epoch #14 Validation Precision: 0.13203463203463203\n",
      "Epoch #15 iteration #50 loss: 0.15673969686031342\n",
      "Epoch #15 iteration #100 loss: 0.12027008831501007\n",
      "Epoch #15 iteration #150 loss: 0.10204625129699707\n",
      "Epoch #15 iteration #200 loss: 0.14602842926979065\n",
      "Epoch #15 iteration #250 loss: 0.13602235913276672\n",
      "Epoch #15 iteration #300 loss: 0.12160248309373856\n",
      "Took 1.762 minutes for epoch# 15 to train\n",
      "Epoch #15 Train loss: 0.13212299291999677\n",
      "Epoch #15 Validation Precision: 0.11351495726495728\n",
      "Epoch #16 iteration #50 loss: 0.12783464789390564\n",
      "Epoch #16 iteration #100 loss: 0.12503841519355774\n",
      "Epoch #16 iteration #150 loss: 0.11769947409629822\n",
      "Epoch #16 iteration #200 loss: 0.16797852516174316\n",
      "Epoch #16 iteration #250 loss: 0.15859122574329376\n",
      "Epoch #16 iteration #300 loss: 0.14450229704380035\n",
      "Took 1.762 minutes for epoch# 16 to train\n",
      "Epoch #16 Train loss: 0.13165615919308785\n",
      "Epoch #16 Validation Precision: 0.0985930735930736\n",
      "Epoch #17 iteration #50 loss: 0.1479879915714264\n",
      "Epoch #17 iteration #100 loss: 0.08478827774524689\n",
      "Epoch #17 iteration #150 loss: 0.11031599342823029\n",
      "Epoch #17 iteration #200 loss: 0.1433141827583313\n",
      "Epoch #17 iteration #250 loss: 0.1570642590522766\n",
      "Epoch #17 iteration #300 loss: 0.16426751017570496\n",
      "Took 1.760 minutes for epoch# 17 to train\n",
      "Epoch #17 Train loss: 0.13023272617600667\n",
      "Epoch #17 Validation Precision: 0.11527777777777777\n",
      "Epoch #18 iteration #50 loss: 0.1725393831729889\n",
      "Epoch #18 iteration #100 loss: 0.11317740380764008\n",
      "Epoch #18 iteration #150 loss: 0.1210232749581337\n",
      "Epoch #18 iteration #200 loss: 0.15495824813842773\n",
      "Epoch #18 iteration #250 loss: 0.16921012103557587\n",
      "Epoch #18 iteration #300 loss: 0.12794646620750427\n",
      "Took 1.763 minutes for epoch# 18 to train\n",
      "Epoch #18 Train loss: 0.12990448077042133\n",
      "Epoch #18 Validation Precision: 0.11587301587301586\n",
      "Epoch #19 iteration #50 loss: 0.15443462133407593\n",
      "Epoch #19 iteration #100 loss: 0.10687386244535446\n",
      "Epoch #19 iteration #150 loss: 0.1101347953081131\n",
      "Epoch #19 iteration #200 loss: 0.1436920166015625\n",
      "Epoch #19 iteration #250 loss: 0.15220347046852112\n",
      "Epoch #19 iteration #300 loss: 0.1254630833864212\n",
      "Took 1.762 minutes for epoch# 19 to train\n",
      "Epoch #19 Train loss: 0.12863380275666714\n",
      "Epoch #19 Validation Precision: 0.11672008547008547\n",
      "Epoch #20 iteration #50 loss: 0.17632046341896057\n",
      "Epoch #20 iteration #100 loss: 0.09171672910451889\n",
      "Epoch #20 iteration #150 loss: 0.11158100515604019\n",
      "Epoch #20 iteration #200 loss: 0.15723471343517303\n",
      "Epoch #20 iteration #250 loss: 0.16003234684467316\n",
      "Epoch #20 iteration #300 loss: 0.13119952380657196\n",
      "Took 1.764 minutes for epoch# 20 to train\n",
      "Epoch #20 Train loss: 0.12682597265125084\n",
      "Epoch #20 Validation Precision: 0.1800595238095238\n",
      "Epoch #21 iteration #50 loss: 0.16887196898460388\n",
      "Epoch #21 iteration #100 loss: 0.09191537648439407\n",
      "Epoch #21 iteration #150 loss: 0.11026931554079056\n",
      "Epoch #21 iteration #200 loss: 0.17949014902114868\n",
      "Epoch #21 iteration #250 loss: 0.1465168297290802\n",
      "Epoch #21 iteration #300 loss: 0.14249400794506073\n",
      "Took 1.763 minutes for epoch# 21 to train\n",
      "Epoch #21 Train loss: 0.12750304724352482\n",
      "Epoch #21 Validation Precision: 0.16101190476190474\n",
      "Epoch #22 iteration #50 loss: 0.17113633453845978\n",
      "Epoch #22 iteration #100 loss: 0.12286095321178436\n",
      "Epoch #22 iteration #150 loss: 0.10355634242296219\n",
      "Epoch #22 iteration #200 loss: 0.15721504390239716\n",
      "Epoch #22 iteration #250 loss: 0.16735729575157166\n",
      "Epoch #22 iteration #300 loss: 0.1483699083328247\n",
      "Took 1.765 minutes for epoch# 22 to train\n",
      "Epoch #22 Train loss: 0.1274313449811859\n",
      "Epoch #22 Validation Precision: 0.17658730158730157\n",
      "Epoch #23 iteration #50 loss: 0.11029600352048874\n",
      "Epoch #23 iteration #100 loss: 0.12438762933015823\n",
      "Epoch #23 iteration #150 loss: 0.12437229603528976\n",
      "Epoch #23 iteration #200 loss: 0.1504477709531784\n",
      "Epoch #23 iteration #250 loss: 0.1542588621377945\n",
      "Epoch #23 iteration #300 loss: 0.1016484871506691\n",
      "Took 1.764 minutes for epoch# 23 to train\n",
      "Epoch #23 Train loss: 0.12615355796729907\n",
      "Epoch #23 Validation Precision: 0.1310185185185185\n",
      "Epoch #24 iteration #50 loss: 0.1592644453048706\n",
      "Epoch #24 iteration #100 loss: 0.1058107316493988\n",
      "Epoch #24 iteration #150 loss: 0.10387445986270905\n",
      "Epoch #24 iteration #200 loss: 0.15479493141174316\n",
      "Epoch #24 iteration #250 loss: 0.16001322865486145\n",
      "Epoch #24 iteration #300 loss: 0.1396660953760147\n",
      "Took 1.766 minutes for epoch# 24 to train\n",
      "Epoch #24 Train loss: 0.12392752791922061\n",
      "Epoch #24 Validation Precision: 0.18988095238095237\n",
      "Epoch #25 iteration #50 loss: 0.14754043519496918\n",
      "Epoch #25 iteration #100 loss: 0.11875533312559128\n",
      "Epoch #25 iteration #150 loss: 0.0835469588637352\n",
      "Epoch #25 iteration #200 loss: 0.17387963831424713\n",
      "Epoch #25 iteration #250 loss: 0.13980160653591156\n",
      "Epoch #25 iteration #300 loss: 0.13015837967395782\n",
      "Took 1.768 minutes for epoch# 25 to train\n",
      "Epoch #25 Train loss: 0.12417120894847009\n",
      "Epoch #25 Validation Precision: 0.13878968253968255\n",
      "Epoch #26 iteration #50 loss: 0.1340058594942093\n",
      "Epoch #26 iteration #100 loss: 0.12755535542964935\n",
      "Epoch #26 iteration #150 loss: 0.12024879455566406\n",
      "Epoch #26 iteration #200 loss: 0.13523969054222107\n",
      "Epoch #26 iteration #250 loss: 0.13486871123313904\n",
      "Epoch #26 iteration #300 loss: 0.13586214184761047\n",
      "Took 1.767 minutes for epoch# 26 to train\n",
      "Epoch #26 Train loss: 0.12229370590872489\n",
      "Epoch #26 Validation Precision: 0.16408730158730156\n",
      "Epoch #27 iteration #50 loss: 0.15112820267677307\n",
      "Epoch #27 iteration #100 loss: 0.09873586893081665\n",
      "Epoch #27 iteration #150 loss: 0.10503508895635605\n",
      "Epoch #27 iteration #200 loss: 0.13240663707256317\n",
      "Epoch #27 iteration #250 loss: 0.13666129112243652\n",
      "Epoch #27 iteration #300 loss: 0.12180569767951965\n",
      "Took 1.768 minutes for epoch# 27 to train\n",
      "Epoch #27 Train loss: 0.12274721494087806\n",
      "Epoch #27 Validation Precision: 0.1454861111111111\n",
      "Epoch #28 iteration #50 loss: 0.1651265174150467\n",
      "Epoch #28 iteration #100 loss: 0.10603366792201996\n",
      "Epoch #28 iteration #150 loss: 0.07905028015375137\n",
      "Epoch #28 iteration #200 loss: 0.1213487982749939\n",
      "Epoch #28 iteration #250 loss: 0.13492855429649353\n",
      "Epoch #28 iteration #300 loss: 0.1268337070941925\n",
      "Took 1.769 minutes for epoch# 28 to train\n",
      "Epoch #28 Train loss: 0.11953691323884787\n",
      "Epoch #28 Validation Precision: 0.17500000000000004\n",
      "Epoch #29 iteration #50 loss: 0.15799903869628906\n",
      "Epoch #29 iteration #100 loss: 0.10946908593177795\n",
      "Epoch #29 iteration #150 loss: 0.09451839327812195\n",
      "Epoch #29 iteration #200 loss: 0.11621975153684616\n",
      "Epoch #29 iteration #250 loss: 0.15283779799938202\n",
      "Epoch #29 iteration #300 loss: 0.1366935819387436\n",
      "Took 1.765 minutes for epoch# 29 to train\n",
      "Epoch #29 Train loss: 0.12000707484399661\n",
      "Epoch #29 Validation Precision: 0.14143518518518516\n",
      "Epoch #30 iteration #50 loss: 0.15791897475719452\n",
      "Epoch #30 iteration #100 loss: 0.10472959280014038\n",
      "Epoch #30 iteration #150 loss: 0.11287935078144073\n",
      "Epoch #30 iteration #200 loss: 0.1211722120642662\n",
      "Epoch #30 iteration #250 loss: 0.16295383870601654\n",
      "Epoch #30 iteration #300 loss: 0.11329691857099533\n",
      "Took 1.769 minutes for epoch# 30 to train\n",
      "Epoch #30 Train loss: 0.11716913990676403\n",
      "Epoch #30 Validation Precision: 0.16349206349206347\n",
      "Epoch #31 iteration #50 loss: 0.16697485744953156\n",
      "Epoch #31 iteration #100 loss: 0.10844799876213074\n",
      "Epoch #31 iteration #150 loss: 0.09576036036014557\n",
      "Epoch #31 iteration #200 loss: 0.1318616420030594\n",
      "Epoch #31 iteration #250 loss: 0.10446770489215851\n",
      "Epoch #31 iteration #300 loss: 0.11480750143527985\n",
      "Took 1.766 minutes for epoch# 31 to train\n",
      "Epoch #31 Train loss: 0.11752605015555254\n",
      "Epoch #31 Validation Precision: 0.19682539682539685\n",
      "Epoch #32 iteration #50 loss: 0.14789603650569916\n",
      "Epoch #32 iteration #100 loss: 0.11646781861782074\n",
      "Epoch #32 iteration #150 loss: 0.07636284828186035\n",
      "Epoch #32 iteration #200 loss: 0.12509311735630035\n",
      "Epoch #32 iteration #250 loss: 0.10346706211566925\n",
      "Epoch #32 iteration #300 loss: 0.11518432945013046\n",
      "Took 1.766 minutes for epoch# 32 to train\n",
      "Epoch #32 Train loss: 0.11618751105971825\n",
      "Epoch #32 Validation Precision: 0.1857142857142857\n",
      "Epoch #33 iteration #50 loss: 0.14719177782535553\n",
      "Epoch #33 iteration #100 loss: 0.099386066198349\n",
      "Epoch #33 iteration #150 loss: 0.11394529044628143\n",
      "Epoch #33 iteration #200 loss: 0.12884521484375\n",
      "Epoch #33 iteration #250 loss: 0.10861373692750931\n",
      "Epoch #33 iteration #300 loss: 0.11045478284358978\n",
      "Took 1.768 minutes for epoch# 33 to train\n",
      "Epoch #33 Train loss: 0.11556889157360181\n",
      "Epoch #33 Validation Precision: 0.2222222222222222\n",
      "Epoch #34 iteration #50 loss: 0.17324860394001007\n",
      "Epoch #34 iteration #100 loss: 0.10925256460905075\n",
      "Epoch #34 iteration #150 loss: 0.09656854718923569\n",
      "Epoch #34 iteration #200 loss: 0.14979293942451477\n",
      "Epoch #34 iteration #250 loss: 0.12839388847351074\n",
      "Epoch #34 iteration #300 loss: 0.12301173806190491\n",
      "Took 1.765 minutes for epoch# 34 to train\n",
      "Epoch #34 Train loss: 0.11512151687668684\n",
      "Epoch #34 Validation Precision: 0.21319444444444444\n",
      "Epoch #35 iteration #50 loss: 0.16443902254104614\n",
      "Epoch #35 iteration #100 loss: 0.10055692493915558\n",
      "Epoch #35 iteration #150 loss: 0.08441959321498871\n",
      "Epoch #35 iteration #200 loss: 0.10673920065164566\n",
      "Epoch #35 iteration #250 loss: 0.13747435808181763\n",
      "Epoch #35 iteration #300 loss: 0.1096368208527565\n",
      "Took 1.767 minutes for epoch# 35 to train\n",
      "Epoch #35 Train loss: 0.11372055001079272\n",
      "Epoch #35 Validation Precision: 0.23015873015873017\n",
      "Epoch #36 iteration #50 loss: 0.13033084571361542\n",
      "Epoch #36 iteration #100 loss: 0.11594655364751816\n",
      "Epoch #36 iteration #150 loss: 0.11031948775053024\n",
      "Epoch #36 iteration #200 loss: 0.09894108027219772\n",
      "Epoch #36 iteration #250 loss: 0.09481610357761383\n",
      "Epoch #36 iteration #300 loss: 0.1150829866528511\n",
      "Took 1.769 minutes for epoch# 36 to train\n",
      "Epoch #36 Train loss: 0.11472236821189141\n",
      "Epoch #36 Validation Precision: 0.24479166666666663\n",
      "Epoch #37 iteration #50 loss: 0.1474154144525528\n",
      "Epoch #37 iteration #100 loss: 0.11518002301454544\n",
      "Epoch #37 iteration #150 loss: 0.128866046667099\n",
      "Epoch #37 iteration #200 loss: 0.11476323008537292\n",
      "Epoch #37 iteration #250 loss: 0.14865976572036743\n",
      "Epoch #37 iteration #300 loss: 0.12134064733982086\n",
      "Took 1.772 minutes for epoch# 37 to train\n",
      "Epoch #37 Train loss: 0.11353215663574445\n",
      "Epoch #37 Validation Precision: 0.2093253968253968\n",
      "Epoch #38 iteration #50 loss: 0.1205202266573906\n",
      "Epoch #38 iteration #100 loss: 0.11038746684789658\n",
      "Epoch #38 iteration #150 loss: 0.09776847064495087\n",
      "Epoch #38 iteration #200 loss: 0.12013016641139984\n",
      "Epoch #38 iteration #250 loss: 0.12404883652925491\n",
      "Epoch #38 iteration #300 loss: 0.12435538321733475\n",
      "Took 1.791 minutes for epoch# 38 to train\n",
      "Epoch #38 Train loss: 0.11259053208124943\n",
      "Epoch #38 Validation Precision: 0.1919642857142857\n",
      "Epoch #39 iteration #50 loss: 0.13430659472942352\n",
      "Epoch #39 iteration #100 loss: 0.10557759553194046\n",
      "Epoch #39 iteration #150 loss: 0.08174291253089905\n",
      "Epoch #39 iteration #200 loss: 0.14911630749702454\n",
      "Epoch #39 iteration #250 loss: 0.11648542433977127\n",
      "Epoch #39 iteration #300 loss: 0.09633918851613998\n",
      "Took 1.804 minutes for epoch# 39 to train\n",
      "Epoch #39 Train loss: 0.11045087534838761\n",
      "Epoch #39 Validation Precision: 0.2579365079365079\n",
      "Epoch #40 iteration #50 loss: 0.17042437195777893\n",
      "Epoch #40 iteration #100 loss: 0.11919739097356796\n",
      "Epoch #40 iteration #150 loss: 0.07712489366531372\n",
      "Epoch #40 iteration #200 loss: 0.13378150761127472\n",
      "Epoch #40 iteration #250 loss: 0.133218452334404\n",
      "Epoch #40 iteration #300 loss: 0.11194004118442535\n",
      "Took 1.785 minutes for epoch# 40 to train\n",
      "Epoch #40 Train loss: 0.10749268610603534\n",
      "Epoch #40 Validation Precision: 0.212797619047619\n",
      "Epoch #41 iteration #50 loss: 0.14818616211414337\n",
      "Epoch #41 iteration #100 loss: 0.09071358293294907\n",
      "Epoch #41 iteration #150 loss: 0.09893494099378586\n",
      "Epoch #41 iteration #200 loss: 0.11329385638237\n",
      "Epoch #41 iteration #250 loss: 0.11090057343244553\n",
      "Epoch #41 iteration #300 loss: 0.11268851161003113\n",
      "Took 1.784 minutes for epoch# 41 to train\n",
      "Epoch #41 Train loss: 0.10991520074029\n",
      "Epoch #41 Validation Precision: 0.20089285714285715\n",
      "Epoch #42 iteration #50 loss: 0.15102678537368774\n",
      "Epoch #42 iteration #100 loss: 0.09436645358800888\n",
      "Epoch #42 iteration #150 loss: 0.08096124231815338\n",
      "Epoch #42 iteration #200 loss: 0.11722803115844727\n",
      "Epoch #42 iteration #250 loss: 0.14978033304214478\n",
      "Epoch #42 iteration #300 loss: 0.09093856811523438\n",
      "Took 1.779 minutes for epoch# 42 to train\n",
      "Epoch #42 Train loss: 0.10923073777499107\n",
      "Epoch #42 Validation Precision: 0.29166666666666663\n",
      "Epoch #43 iteration #50 loss: 0.14462746679782867\n",
      "Epoch #43 iteration #100 loss: 0.0807068794965744\n",
      "Epoch #43 iteration #150 loss: 0.09788728505373001\n",
      "Epoch #43 iteration #200 loss: 0.14736387133598328\n",
      "Epoch #43 iteration #250 loss: 0.10793919861316681\n",
      "Epoch #43 iteration #300 loss: 0.11235789209604263\n",
      "Took 1.773 minutes for epoch# 43 to train\n",
      "Epoch #43 Train loss: 0.10854466796781008\n",
      "Epoch #43 Validation Precision: 0.2541666666666667\n",
      "Epoch #44 iteration #50 loss: 0.1321781873703003\n",
      "Epoch #44 iteration #100 loss: 0.08656547218561172\n",
      "Epoch #44 iteration #150 loss: 0.06402759253978729\n",
      "Epoch #44 iteration #200 loss: 0.11808838695287704\n",
      "Epoch #44 iteration #250 loss: 0.11832690984010696\n",
      "Epoch #44 iteration #300 loss: 0.10647425055503845\n",
      "Took 1.769 minutes for epoch# 44 to train\n",
      "Epoch #44 Train loss: 0.10659852640655561\n",
      "Epoch #44 Validation Precision: 0.22708333333333336\n",
      "Epoch #45 iteration #50 loss: 0.13383543491363525\n",
      "Epoch #45 iteration #100 loss: 0.11288567632436752\n",
      "Epoch #45 iteration #150 loss: 0.08219254016876221\n",
      "Epoch #45 iteration #200 loss: 0.11144089698791504\n",
      "Epoch #45 iteration #250 loss: 0.09240207076072693\n",
      "Epoch #45 iteration #300 loss: 0.11401310563087463\n",
      "Took 1.769 minutes for epoch# 45 to train\n",
      "Epoch #45 Train loss: 0.10551278407757099\n",
      "Epoch #45 Validation Precision: 0.18402777777777773\n",
      "Epoch #46 iteration #50 loss: 0.13162925839424133\n",
      "Epoch #46 iteration #100 loss: 0.08719325810670853\n",
      "Epoch #46 iteration #150 loss: 0.09093091636896133\n",
      "Epoch #46 iteration #200 loss: 0.11070238053798676\n",
      "Epoch #46 iteration #250 loss: 0.10171452909708023\n",
      "Epoch #46 iteration #300 loss: 0.10482494533061981\n",
      "Took 1.769 minutes for epoch# 46 to train\n",
      "Epoch #46 Train loss: 0.10540242641209027\n",
      "Epoch #46 Validation Precision: 0.24814814814814812\n",
      "Epoch #47 iteration #50 loss: 0.12944218516349792\n",
      "Epoch #47 iteration #100 loss: 0.09754364937543869\n",
      "Epoch #47 iteration #150 loss: 0.07155958563089371\n",
      "Epoch #47 iteration #200 loss: 0.11096250265836716\n",
      "Epoch #47 iteration #250 loss: 0.11703160405158997\n",
      "Epoch #47 iteration #300 loss: 0.1581919938325882\n",
      "Took 1.766 minutes for epoch# 47 to train\n",
      "Epoch #47 Train loss: 0.10414209484289853\n",
      "Epoch #47 Validation Precision: 0.20833333333333331\n",
      "Epoch #48 iteration #50 loss: 0.13174614310264587\n",
      "Epoch #48 iteration #100 loss: 0.08866696804761887\n",
      "Epoch #48 iteration #150 loss: 0.07916323840618134\n",
      "Epoch #48 iteration #200 loss: 0.1272696554660797\n",
      "Epoch #48 iteration #250 loss: 0.0809253454208374\n",
      "Epoch #48 iteration #300 loss: 0.11189727485179901\n",
      "Took 1.769 minutes for epoch# 48 to train\n",
      "Epoch #48 Train loss: 0.10319240615727046\n",
      "Epoch #48 Validation Precision: 0.15476190476190477\n",
      "Epoch #49 iteration #50 loss: 0.12659461796283722\n",
      "Epoch #49 iteration #100 loss: 0.10887307673692703\n",
      "Epoch #49 iteration #150 loss: 0.06868906319141388\n",
      "Epoch #49 iteration #200 loss: 0.09885919094085693\n",
      "Epoch #49 iteration #250 loss: 0.11506287008523941\n",
      "Epoch #49 iteration #300 loss: 0.09591355919837952\n",
      "Took 1.777 minutes for epoch# 49 to train\n",
      "Epoch #49 Train loss: 0.10153479513545068\n",
      "Epoch #49 Validation Precision: 0.20654761904761904\n",
      "Epoch #50 iteration #50 loss: 0.1600555032491684\n",
      "Epoch #50 iteration #100 loss: 0.11928756535053253\n",
      "Epoch #50 iteration #150 loss: 0.08620680123567581\n",
      "Epoch #50 iteration #200 loss: 0.100114606320858\n",
      "Epoch #50 iteration #250 loss: 0.09309984743595123\n",
      "Epoch #50 iteration #300 loss: 0.0943298265337944\n",
      "Took 1.774 minutes for epoch# 50 to train\n",
      "Epoch #50 Train loss: 0.10200816641251247\n",
      "Epoch #50 Validation Precision: 0.24444444444444446\n",
      "Epoch #51 iteration #50 loss: 0.12594850361347198\n",
      "Epoch #51 iteration #100 loss: 0.09863969683647156\n",
      "Epoch #51 iteration #150 loss: 0.08079768717288971\n",
      "Epoch #51 iteration #200 loss: 0.09800895303487778\n",
      "Epoch #51 iteration #250 loss: 0.12270507961511612\n",
      "Epoch #51 iteration #300 loss: 0.10926786065101624\n",
      "Took 1.770 minutes for epoch# 51 to train\n",
      "Epoch #51 Train loss: 0.10095327940936653\n",
      "Epoch #51 Validation Precision: 0.19633838383838384\n",
      "Epoch #52 iteration #50 loss: 0.14858156442642212\n",
      "Epoch #52 iteration #100 loss: 0.09986671805381775\n",
      "Epoch #52 iteration #150 loss: 0.07032950222492218\n",
      "Epoch #52 iteration #200 loss: 0.09858293831348419\n",
      "Epoch #52 iteration #250 loss: 0.10570970922708511\n",
      "Epoch #52 iteration #300 loss: 0.09072759747505188\n",
      "Took 1.771 minutes for epoch# 52 to train\n",
      "Epoch #52 Train loss: 0.10139364381440175\n",
      "Epoch #52 Validation Precision: 0.1919642857142857\n",
      "Epoch #53 iteration #50 loss: 0.14170882105827332\n",
      "Epoch #53 iteration #100 loss: 0.08350297808647156\n",
      "Epoch #53 iteration #150 loss: 0.08547287434339523\n",
      "Epoch #53 iteration #200 loss: 0.10820546746253967\n",
      "Epoch #53 iteration #250 loss: 0.09668630361557007\n",
      "Epoch #53 iteration #300 loss: 0.107341468334198\n",
      "Took 1.769 minutes for epoch# 53 to train\n",
      "Epoch #53 Train loss: 0.10016236029183254\n",
      "Epoch #53 Validation Precision: 0.1950892857142857\n",
      "Epoch #54 iteration #50 loss: 0.11887191981077194\n",
      "Epoch #54 iteration #100 loss: 0.11811639368534088\n",
      "Epoch #54 iteration #150 loss: 0.0896306112408638\n",
      "Epoch #54 iteration #200 loss: 0.1020454466342926\n",
      "Epoch #54 iteration #250 loss: 0.1022123470902443\n",
      "Epoch #54 iteration #300 loss: 0.09617689251899719\n",
      "Took 1.772 minutes for epoch# 54 to train\n",
      "Epoch #54 Train loss: 0.09870901666820431\n",
      "Epoch #54 Validation Precision: 0.2875\n",
      "Epoch #55 iteration #50 loss: 0.12391259521245956\n",
      "Epoch #55 iteration #100 loss: 0.12966203689575195\n",
      "Epoch #55 iteration #150 loss: 0.06564725190401077\n",
      "Epoch #55 iteration #200 loss: 0.10176759958267212\n",
      "Epoch #55 iteration #250 loss: 0.09056197106838226\n",
      "Epoch #55 iteration #300 loss: 0.10402489453554153\n",
      "Took 1.769 minutes for epoch# 55 to train\n",
      "Epoch #55 Train loss: 0.09801682768962704\n",
      "Epoch #55 Validation Precision: 0.2569444444444444\n",
      "Epoch #56 iteration #50 loss: 0.1282687783241272\n",
      "Epoch #56 iteration #100 loss: 0.08417551219463348\n",
      "Epoch #56 iteration #150 loss: 0.06435148417949677\n",
      "Epoch #56 iteration #200 loss: 0.16166254878044128\n",
      "Epoch #56 iteration #250 loss: 0.11724404245615005\n",
      "Epoch #56 iteration #300 loss: 0.10941311717033386\n",
      "Took 1.768 minutes for epoch# 56 to train\n",
      "Epoch #56 Train loss: 0.09813026142999148\n",
      "Epoch #56 Validation Precision: 0.2569444444444445\n",
      "Epoch #57 iteration #50 loss: 0.11480827629566193\n",
      "Epoch #57 iteration #100 loss: 0.10585391521453857\n",
      "Epoch #57 iteration #150 loss: 0.08986060321331024\n",
      "Epoch #57 iteration #200 loss: 0.07611462473869324\n",
      "Epoch #57 iteration #250 loss: 0.10481132566928864\n",
      "Epoch #57 iteration #300 loss: 0.12144306302070618\n",
      "Took 1.771 minutes for epoch# 57 to train\n",
      "Epoch #57 Train loss: 0.09507842259242749\n",
      "Epoch #57 Validation Precision: 0.25\n",
      "Epoch #58 iteration #50 loss: 0.09884250164031982\n",
      "Epoch #58 iteration #100 loss: 0.088673435151577\n",
      "Epoch #58 iteration #150 loss: 0.07649001479148865\n",
      "Epoch #58 iteration #200 loss: 0.11181813478469849\n",
      "Epoch #58 iteration #250 loss: 0.09049413353204727\n",
      "Epoch #58 iteration #300 loss: 0.10917768627405167\n",
      "Took 1.774 minutes for epoch# 58 to train\n",
      "Epoch #58 Train loss: 0.09633742303897937\n",
      "Epoch #58 Validation Precision: 0.2652777777777777\n",
      "Epoch #59 iteration #50 loss: 0.0912875160574913\n",
      "Epoch #59 iteration #100 loss: 0.0893176719546318\n",
      "Epoch #59 iteration #150 loss: 0.07362934947013855\n",
      "Epoch #59 iteration #200 loss: 0.08701493591070175\n",
      "Epoch #59 iteration #250 loss: 0.08569102734327316\n",
      "Epoch #59 iteration #300 loss: 0.10614407062530518\n",
      "Took 1.773 minutes for epoch# 59 to train\n",
      "Epoch #59 Train loss: 0.09471554789119042\n",
      "Epoch #59 Validation Precision: 0.375\n",
      "Epoch #60 iteration #50 loss: 0.10319299250841141\n",
      "Epoch #60 iteration #100 loss: 0.08084412664175034\n",
      "Epoch #60 iteration #150 loss: 0.06981588155031204\n",
      "Epoch #60 iteration #200 loss: 0.09590252488851547\n",
      "Epoch #60 iteration #250 loss: 0.09079528599977493\n",
      "Epoch #60 iteration #300 loss: 0.10087519884109497\n",
      "Took 1.776 minutes for epoch# 60 to train\n",
      "Epoch #60 Train loss: 0.09343110220745587\n",
      "Epoch #60 Validation Precision: 0.2583333333333333\n",
      "Epoch #61 iteration #50 loss: 0.15138834714889526\n",
      "Epoch #61 iteration #100 loss: 0.07151667028665543\n",
      "Epoch #61 iteration #150 loss: 0.07763934880495071\n",
      "Epoch #61 iteration #200 loss: 0.14800067245960236\n",
      "Epoch #61 iteration #250 loss: 0.11314772069454193\n",
      "Epoch #61 iteration #300 loss: 0.08636641502380371\n",
      "Took 1.775 minutes for epoch# 61 to train\n",
      "Epoch #61 Train loss: 0.09349853514383237\n",
      "Epoch #61 Validation Precision: 0.34375\n",
      "Epoch #62 iteration #50 loss: 0.10445472598075867\n",
      "Epoch #62 iteration #100 loss: 0.09815341234207153\n",
      "Epoch #62 iteration #150 loss: 0.07288632541894913\n",
      "Epoch #62 iteration #200 loss: 0.10418353229761124\n",
      "Epoch #62 iteration #250 loss: 0.09543643146753311\n",
      "Epoch #62 iteration #300 loss: 0.10526445508003235\n",
      "Took 1.774 minutes for epoch# 62 to train\n",
      "Epoch #62 Train loss: 0.09263017156328528\n",
      "Epoch #62 Validation Precision: 0.45\n",
      "Epoch #63 iteration #50 loss: 0.09742846339941025\n",
      "Epoch #63 iteration #100 loss: 0.0980561375617981\n",
      "Epoch #63 iteration #150 loss: 0.08595946431159973\n",
      "Epoch #63 iteration #200 loss: 0.10145383328199387\n",
      "Epoch #63 iteration #250 loss: 0.08475269377231598\n",
      "Epoch #63 iteration #300 loss: 0.13108833134174347\n",
      "Took 1.776 minutes for epoch# 63 to train\n",
      "Epoch #63 Train loss: 0.09068365750882106\n",
      "Epoch #63 Validation Precision: 0.41898148148148145\n",
      "Epoch #64 iteration #50 loss: 0.0990070104598999\n",
      "Epoch #64 iteration #100 loss: 0.09431774914264679\n",
      "Epoch #64 iteration #150 loss: 0.06517430394887924\n",
      "Epoch #64 iteration #200 loss: 0.09141121804714203\n",
      "Epoch #64 iteration #250 loss: 0.09961112588644028\n",
      "Epoch #64 iteration #300 loss: 0.11072614043951035\n",
      "Took 1.772 minutes for epoch# 64 to train\n",
      "Epoch #64 Train loss: 0.09066759525105739\n",
      "Epoch #64 Validation Precision: 0.369047619047619\n",
      "Epoch #65 iteration #50 loss: 0.10536117851734161\n",
      "Epoch #65 iteration #100 loss: 0.08002657443284988\n",
      "Epoch #65 iteration #150 loss: 0.05692186579108238\n",
      "Epoch #65 iteration #200 loss: 0.09064608067274094\n",
      "Epoch #65 iteration #250 loss: 0.10302963852882385\n",
      "Epoch #65 iteration #300 loss: 0.08869152516126633\n",
      "Took 1.774 minutes for epoch# 65 to train\n",
      "Epoch #65 Train loss: 0.0888384480554706\n",
      "Epoch #65 Validation Precision: 0.38541666666666663\n",
      "Epoch #66 iteration #50 loss: 0.1501338630914688\n",
      "Epoch #66 iteration #100 loss: 0.08337147533893585\n",
      "Epoch #66 iteration #150 loss: 0.06186201795935631\n",
      "Epoch #66 iteration #200 loss: 0.13119064271450043\n",
      "Epoch #66 iteration #250 loss: 0.06047944724559784\n",
      "Epoch #66 iteration #300 loss: 0.12117954343557358\n",
      "Took 1.772 minutes for epoch# 66 to train\n",
      "Epoch #66 Train loss: 0.088189551487374\n",
      "Epoch #66 Validation Precision: 0.5104166666666666\n",
      "Epoch #67 iteration #50 loss: 0.17126354575157166\n",
      "Epoch #67 iteration #100 loss: 0.07863892614841461\n",
      "Epoch #67 iteration #150 loss: 0.06658667325973511\n",
      "Epoch #67 iteration #200 loss: 0.10980799794197083\n",
      "Epoch #67 iteration #250 loss: 0.07579918205738068\n",
      "Epoch #67 iteration #300 loss: 0.14815475046634674\n",
      "Took 1.775 minutes for epoch# 67 to train\n",
      "Epoch #67 Train loss: 0.08710285012299816\n",
      "Epoch #67 Validation Precision: 0.5104166666666666\n",
      "Epoch #68 iteration #50 loss: 0.104683518409729\n",
      "Epoch #68 iteration #100 loss: 0.11194873601198196\n",
      "Epoch #68 iteration #150 loss: 0.07612526416778564\n",
      "Epoch #68 iteration #200 loss: 0.07879145443439484\n",
      "Epoch #68 iteration #250 loss: 0.09313064813613892\n",
      "Epoch #68 iteration #300 loss: 0.10994800180196762\n",
      "Took 1.775 minutes for epoch# 68 to train\n",
      "Epoch #68 Train loss: 0.08876780536360084\n",
      "Epoch #68 Validation Precision: 0.3583333333333333\n",
      "Epoch #69 iteration #50 loss: 0.10571631044149399\n",
      "Epoch #69 iteration #100 loss: 0.07701683789491653\n",
      "Epoch #69 iteration #150 loss: 0.08366373181343079\n",
      "Epoch #69 iteration #200 loss: 0.0911066085100174\n",
      "Epoch #69 iteration #250 loss: 0.07619854807853699\n",
      "Epoch #69 iteration #300 loss: 0.09795603901147842\n",
      "Took 1.775 minutes for epoch# 69 to train\n",
      "Epoch #69 Train loss: 0.08738628830999517\n",
      "Epoch #69 Validation Precision: 0.5249999999999999\n",
      "Epoch #70 iteration #50 loss: 0.11027294397354126\n",
      "Epoch #70 iteration #100 loss: 0.08359971642494202\n",
      "Epoch #70 iteration #150 loss: 0.05966468155384064\n",
      "Epoch #70 iteration #200 loss: 0.07297904044389725\n",
      "Epoch #70 iteration #250 loss: 0.08368143439292908\n",
      "Epoch #70 iteration #300 loss: 0.13181890547275543\n",
      "Took 1.774 minutes for epoch# 70 to train\n",
      "Epoch #70 Train loss: 0.08577386228940807\n",
      "Epoch #70 Validation Precision: 0.48749999999999993\n",
      "Epoch #71 iteration #50 loss: 0.09113600105047226\n",
      "Epoch #71 iteration #100 loss: 0.07971035689115524\n",
      "Epoch #71 iteration #150 loss: 0.08383256942033768\n",
      "Epoch #71 iteration #200 loss: 0.10701973736286163\n",
      "Epoch #71 iteration #250 loss: 0.07218822091817856\n",
      "Epoch #71 iteration #300 loss: 0.11061330139636993\n",
      "Took 1.773 minutes for epoch# 71 to train\n",
      "Epoch #71 Train loss: 0.08709074449367248\n",
      "Epoch #71 Validation Precision: 0.2625\n",
      "Epoch #72 iteration #50 loss: 0.11879567801952362\n",
      "Epoch #72 iteration #100 loss: 0.09236171841621399\n",
      "Epoch #72 iteration #150 loss: 0.06803839653730392\n",
      "Epoch #72 iteration #200 loss: 0.07199437916278839\n",
      "Epoch #72 iteration #250 loss: 0.08422107994556427\n",
      "Epoch #72 iteration #300 loss: 0.08916471153497696\n",
      "Took 1.774 minutes for epoch# 72 to train\n",
      "Epoch #72 Train loss: 0.08691625584824345\n",
      "Epoch #72 Validation Precision: 0.21726190476190477\n",
      "Epoch #73 iteration #50 loss: 0.10367513447999954\n",
      "Epoch #73 iteration #100 loss: 0.08805883675813675\n",
      "Epoch #73 iteration #150 loss: 0.0671321302652359\n",
      "Epoch #73 iteration #200 loss: 0.12054808437824249\n",
      "Epoch #73 iteration #250 loss: 0.07686476409435272\n",
      "Epoch #73 iteration #300 loss: 0.08021004498004913\n",
      "Took 1.774 minutes for epoch# 73 to train\n",
      "Epoch #73 Train loss: 0.08577665797649668\n",
      "Epoch #73 Validation Precision: 0.275297619047619\n",
      "Epoch #74 iteration #50 loss: 0.0914992243051529\n",
      "Epoch #74 iteration #100 loss: 0.07190240919589996\n",
      "Epoch #74 iteration #150 loss: 0.07144342362880707\n",
      "Epoch #74 iteration #200 loss: 0.10026074200868607\n",
      "Epoch #74 iteration #250 loss: 0.09260613471269608\n",
      "Epoch #74 iteration #300 loss: 0.07404818385839462\n",
      "Took 1.774 minutes for epoch# 74 to train\n",
      "Epoch #74 Train loss: 0.08271455018518445\n",
      "Epoch #74 Validation Precision: 0.4041666666666666\n",
      "Epoch #75 iteration #50 loss: 0.09954910725355148\n",
      "Epoch #75 iteration #100 loss: 0.06385832279920578\n",
      "Epoch #75 iteration #150 loss: 0.07270468771457672\n",
      "Epoch #75 iteration #200 loss: 0.08235505223274231\n",
      "Epoch #75 iteration #250 loss: 0.07315795123577118\n",
      "Epoch #75 iteration #300 loss: 0.08751554787158966\n",
      "Took 1.779 minutes for epoch# 75 to train\n",
      "Epoch #75 Train loss: 0.08322326118986194\n",
      "Epoch #75 Validation Precision: 0.4652777777777778\n",
      "Epoch #76 iteration #50 loss: 0.11653841286897659\n",
      "Epoch #76 iteration #100 loss: 0.07959526032209396\n",
      "Epoch #76 iteration #150 loss: 0.07764368504285812\n",
      "Epoch #76 iteration #200 loss: 0.09458983689546585\n",
      "Epoch #76 iteration #250 loss: 0.10815488547086716\n",
      "Epoch #76 iteration #300 loss: 0.09644825011491776\n",
      "Took 1.803 minutes for epoch# 76 to train\n",
      "Epoch #76 Train loss: 0.08398195670153467\n",
      "Epoch #76 Validation Precision: 0.42708333333333337\n",
      "Epoch #77 iteration #50 loss: 0.0793122947216034\n",
      "Epoch #77 iteration #100 loss: 0.07960821688175201\n",
      "Epoch #77 iteration #150 loss: 0.06362049281597137\n",
      "Epoch #77 iteration #200 loss: 0.09114279597997665\n",
      "Epoch #77 iteration #250 loss: 0.10686151683330536\n",
      "Epoch #77 iteration #300 loss: 0.10848402976989746\n",
      "Took 1.792 minutes for epoch# 77 to train\n",
      "Epoch #77 Train loss: 0.08244418822085628\n",
      "Epoch #77 Validation Precision: 0.5208333333333333\n",
      "Epoch #78 iteration #50 loss: 0.09054498374462128\n",
      "Epoch #78 iteration #100 loss: 0.12493404746055603\n",
      "Epoch #78 iteration #150 loss: 0.06509619951248169\n",
      "Epoch #78 iteration #200 loss: 0.09189377725124359\n",
      "Epoch #78 iteration #250 loss: 0.09190455079078674\n",
      "Epoch #78 iteration #300 loss: 0.09063538908958435\n",
      "Took 1.795 minutes for epoch# 78 to train\n",
      "Epoch #78 Train loss: 0.08166527232298484\n",
      "Epoch #78 Validation Precision: 0.4333333333333333\n",
      "Epoch #79 iteration #50 loss: 0.11310333758592606\n",
      "Epoch #79 iteration #100 loss: 0.08673328161239624\n",
      "Epoch #79 iteration #150 loss: 0.07553230226039886\n",
      "Epoch #79 iteration #200 loss: 0.06702414900064468\n",
      "Epoch #79 iteration #250 loss: 0.07403495162725449\n",
      "Epoch #79 iteration #300 loss: 0.10706992447376251\n",
      "Took 1.780 minutes for epoch# 79 to train\n",
      "Epoch #79 Train loss: 0.08367939043837862\n",
      "Epoch #79 Validation Precision: 0.3169642857142857\n",
      "Epoch #80 iteration #50 loss: 0.10279355198144913\n",
      "Epoch #80 iteration #100 loss: 0.06427823752164841\n",
      "Epoch #80 iteration #150 loss: 0.06656797975301743\n",
      "Epoch #80 iteration #200 loss: 0.11400861293077469\n",
      "Epoch #80 iteration #250 loss: 0.06594537943601608\n",
      "Epoch #80 iteration #300 loss: 0.07797738909721375\n",
      "Took 1.778 minutes for epoch# 80 to train\n",
      "Epoch #80 Train loss: 0.08257297341687939\n",
      "Epoch #80 Validation Precision: 0.5430555555555555\n",
      "Epoch #81 iteration #50 loss: 0.11457806080579758\n",
      "Epoch #81 iteration #100 loss: 0.07222969830036163\n",
      "Epoch #81 iteration #150 loss: 0.06685006618499756\n",
      "Epoch #81 iteration #200 loss: 0.07698619365692139\n",
      "Epoch #81 iteration #250 loss: 0.08530164510011673\n",
      "Epoch #81 iteration #300 loss: 0.08872061967849731\n",
      "Took 1.783 minutes for epoch# 81 to train\n",
      "Epoch #81 Train loss: 0.08131531769266495\n",
      "Epoch #81 Validation Precision: 0.35416666666666663\n",
      "Epoch #82 iteration #50 loss: 0.09255322813987732\n",
      "Epoch #82 iteration #100 loss: 0.0903935357928276\n",
      "Epoch #82 iteration #150 loss: 0.04812637344002724\n",
      "Epoch #82 iteration #200 loss: 0.07930977642536163\n",
      "Epoch #82 iteration #250 loss: 0.08276426792144775\n",
      "Epoch #82 iteration #300 loss: 0.11118369549512863\n",
      "Took 1.778 minutes for epoch# 82 to train\n",
      "Epoch #82 Train loss: 0.0807080390289999\n",
      "Epoch #82 Validation Precision: 0.3260416666666666\n",
      "Epoch #83 iteration #50 loss: 0.10798937827348709\n",
      "Epoch #83 iteration #100 loss: 0.07293356955051422\n",
      "Epoch #83 iteration #150 loss: 0.07701385021209717\n",
      "Epoch #83 iteration #200 loss: 0.08108768612146378\n",
      "Epoch #83 iteration #250 loss: 0.08155234903097153\n",
      "Epoch #83 iteration #300 loss: 0.06457509845495224\n",
      "Took 1.779 minutes for epoch# 83 to train\n",
      "Epoch #83 Train loss: 0.08076585362402675\n",
      "Epoch #83 Validation Precision: 0.47916666666666663\n",
      "Epoch #84 iteration #50 loss: 0.09571944922208786\n",
      "Epoch #84 iteration #100 loss: 0.08390525728464127\n",
      "Epoch #84 iteration #150 loss: 0.049011316150426865\n",
      "Epoch #84 iteration #200 loss: 0.08095256239175797\n",
      "Epoch #84 iteration #250 loss: 0.06233764439821243\n",
      "Epoch #84 iteration #300 loss: 0.07610883563756943\n",
      "Took 1.778 minutes for epoch# 84 to train\n",
      "Epoch #84 Train loss: 0.0778109015395435\n",
      "Epoch #84 Validation Precision: 0.4333333333333333\n",
      "Epoch #85 iteration #50 loss: 0.11628396064043045\n",
      "Epoch #85 iteration #100 loss: 0.08040482550859451\n",
      "Epoch #85 iteration #150 loss: 0.06491923332214355\n",
      "Epoch #85 iteration #200 loss: 0.08976827561855316\n",
      "Epoch #85 iteration #250 loss: 0.10476411879062653\n",
      "Epoch #85 iteration #300 loss: 0.06948865205049515\n",
      "Took 1.775 minutes for epoch# 85 to train\n",
      "Epoch #85 Train loss: 0.07848205148743895\n",
      "Epoch #85 Validation Precision: 0.3416666666666667\n",
      "Epoch #86 iteration #50 loss: 0.09618707746267319\n",
      "Epoch #86 iteration #100 loss: 0.06745962798595428\n",
      "Epoch #86 iteration #150 loss: 0.09143754839897156\n",
      "Epoch #86 iteration #200 loss: 0.07420630007982254\n",
      "Epoch #86 iteration #250 loss: 0.09470780938863754\n",
      "Epoch #86 iteration #300 loss: 0.09536977857351303\n",
      "Took 1.779 minutes for epoch# 86 to train\n",
      "Epoch #86 Train loss: 0.07756737069202921\n",
      "Epoch #86 Validation Precision: 0.29682539682539677\n",
      "Epoch #87 iteration #50 loss: 0.07972219586372375\n",
      "Epoch #87 iteration #100 loss: 0.06804357469081879\n",
      "Epoch #87 iteration #150 loss: 0.07427696138620377\n",
      "Epoch #87 iteration #200 loss: 0.08892723172903061\n",
      "Epoch #87 iteration #250 loss: 0.07053494453430176\n",
      "Epoch #87 iteration #300 loss: 0.08710580319166183\n",
      "Took 1.776 minutes for epoch# 87 to train\n",
      "Epoch #87 Train loss: 0.07729901320850238\n",
      "Epoch #87 Validation Precision: 0.44814814814814813\n",
      "Epoch #88 iteration #50 loss: 0.08496483415365219\n",
      "Epoch #88 iteration #100 loss: 0.0752430260181427\n",
      "Epoch #88 iteration #150 loss: 0.06419265270233154\n",
      "Epoch #88 iteration #200 loss: 0.07489137351512909\n",
      "Epoch #88 iteration #250 loss: 0.10165493190288544\n",
      "Epoch #88 iteration #300 loss: 0.07800494134426117\n",
      "Took 1.798 minutes for epoch# 88 to train\n",
      "Epoch #88 Train loss: 0.07706022553910048\n",
      "Epoch #88 Validation Precision: 0.4749999999999999\n",
      "Epoch #89 iteration #50 loss: 0.07378759235143661\n",
      "Epoch #89 iteration #100 loss: 0.08444501459598541\n",
      "Epoch #89 iteration #150 loss: 0.06398909538984299\n",
      "Epoch #89 iteration #200 loss: 0.07726968824863434\n",
      "Epoch #89 iteration #250 loss: 0.08005454391241074\n",
      "Epoch #89 iteration #300 loss: 0.08333467692136765\n",
      "Took 1.883 minutes for epoch# 89 to train\n",
      "Epoch #89 Train loss: 0.07552126331780201\n",
      "Epoch #89 Validation Precision: 0.4404761904761904\n",
      "Epoch #90 iteration #50 loss: 0.09181023389101028\n",
      "Epoch #90 iteration #100 loss: 0.07747936248779297\n",
      "Epoch #90 iteration #150 loss: 0.0704619362950325\n",
      "Epoch #90 iteration #200 loss: 0.07092877477407455\n",
      "Epoch #90 iteration #250 loss: 0.09403304010629654\n",
      "Epoch #90 iteration #300 loss: 0.09370037168264389\n",
      "Took 1.982 minutes for epoch# 90 to train\n",
      "Epoch #90 Train loss: 0.07650613826580155\n",
      "Epoch #90 Validation Precision: 0.19404761904761902\n",
      "Epoch #91 iteration #50 loss: 0.08700983226299286\n",
      "Epoch #91 iteration #100 loss: 0.06707856059074402\n",
      "Epoch #91 iteration #150 loss: 0.09067604690790176\n",
      "Epoch #91 iteration #200 loss: 0.07558717578649521\n",
      "Epoch #91 iteration #250 loss: 0.06787988543510437\n",
      "Epoch #91 iteration #300 loss: 0.08119750022888184\n",
      "Took 1.978 minutes for epoch# 91 to train\n",
      "Epoch #91 Train loss: 0.0744693733465213\n",
      "Epoch #91 Validation Precision: 0.43181818181818177\n",
      "Epoch #92 iteration #50 loss: 0.10734064131975174\n",
      "Epoch #92 iteration #100 loss: 0.0967508852481842\n",
      "Epoch #92 iteration #150 loss: 0.07148391008377075\n",
      "Epoch #92 iteration #200 loss: 0.09016253799200058\n",
      "Epoch #92 iteration #250 loss: 0.07623710483312607\n",
      "Epoch #92 iteration #300 loss: 0.07774758338928223\n",
      "Took 1.828 minutes for epoch# 92 to train\n",
      "Epoch #92 Train loss: 0.0754702751023265\n",
      "Epoch #92 Validation Precision: 0.45833333333333326\n",
      "Epoch #93 iteration #50 loss: 0.08287160098552704\n",
      "Epoch #93 iteration #100 loss: 0.06926542520523071\n",
      "Epoch #93 iteration #150 loss: 0.06856650859117508\n",
      "Epoch #93 iteration #200 loss: 0.08353341370820999\n",
      "Epoch #93 iteration #250 loss: 0.07851333916187286\n",
      "Epoch #93 iteration #300 loss: 0.07992452383041382\n",
      "Took 1.779 minutes for epoch# 93 to train\n",
      "Epoch #93 Train loss: 0.07438793746181406\n",
      "Epoch #93 Validation Precision: 0.48749999999999993\n",
      "Epoch #94 iteration #50 loss: 0.09132885932922363\n",
      "Epoch #94 iteration #100 loss: 0.07322648912668228\n",
      "Epoch #94 iteration #150 loss: 0.056995999068021774\n",
      "Epoch #94 iteration #200 loss: 0.09323109686374664\n",
      "Epoch #94 iteration #250 loss: 0.052745744585990906\n",
      "Epoch #94 iteration #300 loss: 0.0644911527633667\n",
      "Took 1.778 minutes for epoch# 94 to train\n",
      "Epoch #94 Train loss: 0.07350046002568725\n",
      "Epoch #94 Validation Precision: 0.47435897435897434\n",
      "Epoch #95 iteration #50 loss: 0.0845654159784317\n",
      "Epoch #95 iteration #100 loss: 0.07181118428707123\n",
      "Epoch #95 iteration #150 loss: 0.06385128945112228\n",
      "Epoch #95 iteration #200 loss: 0.09499673545360565\n",
      "Epoch #95 iteration #250 loss: 0.08072007447481155\n",
      "Epoch #95 iteration #300 loss: 0.07644454389810562\n",
      "Took 1.793 minutes for epoch# 95 to train\n",
      "Epoch #95 Train loss: 0.0722458841971671\n",
      "Epoch #95 Validation Precision: 0.5763888888888888\n",
      "Epoch #96 iteration #50 loss: 0.0995318591594696\n",
      "Epoch #96 iteration #100 loss: 0.05170407146215439\n",
      "Epoch #96 iteration #150 loss: 0.1058892235159874\n",
      "Epoch #96 iteration #200 loss: 0.08717075735330582\n",
      "Epoch #96 iteration #250 loss: 0.10536623001098633\n",
      "Epoch #96 iteration #300 loss: 0.056799232959747314\n",
      "Took 1.798 minutes for epoch# 96 to train\n",
      "Epoch #96 Train loss: 0.07131698868500116\n",
      "Epoch #96 Validation Precision: 0.3472222222222222\n",
      "Epoch #97 iteration #50 loss: 0.10688071697950363\n",
      "Epoch #97 iteration #100 loss: 0.05363506078720093\n",
      "Epoch #97 iteration #150 loss: 0.05812380835413933\n",
      "Epoch #97 iteration #200 loss: 0.08235187828540802\n",
      "Epoch #97 iteration #250 loss: 0.05979539826512337\n",
      "Epoch #97 iteration #300 loss: 0.08001141995191574\n",
      "Took 1.878 minutes for epoch# 97 to train\n",
      "Epoch #97 Train loss: 0.07253777240522397\n",
      "Epoch #97 Validation Precision: 0.4354166666666666\n",
      "Epoch #98 iteration #50 loss: 0.08699743449687958\n",
      "Epoch #98 iteration #100 loss: 0.06581012159585953\n",
      "Epoch #98 iteration #150 loss: 0.046295102685689926\n",
      "Epoch #98 iteration #200 loss: 0.06383107602596283\n",
      "Epoch #98 iteration #250 loss: 0.07017374783754349\n",
      "Epoch #98 iteration #300 loss: 0.06409934908151627\n",
      "Took 1.907 minutes for epoch# 98 to train\n",
      "Epoch #98 Train loss: 0.0724610039152396\n",
      "Epoch #98 Validation Precision: 0.4722222222222222\n",
      "Epoch #99 iteration #50 loss: 0.06381291151046753\n",
      "Epoch #99 iteration #100 loss: 0.07655491679906845\n",
      "Epoch #99 iteration #150 loss: 0.06681158393621445\n",
      "Epoch #99 iteration #200 loss: 0.06156674399971962\n",
      "Epoch #99 iteration #250 loss: 0.06540905684232712\n",
      "Epoch #99 iteration #300 loss: 0.07337845861911774\n",
      "Took 1.780 minutes for epoch# 99 to train\n",
      "Epoch #99 Train loss: 0.0710322177873399\n",
      "Epoch #99 Validation Precision: 0.24999999999999997\n",
      "Epoch #100 iteration #50 loss: 0.09019831568002701\n",
      "Epoch #100 iteration #100 loss: 0.07348544150590897\n",
      "Epoch #100 iteration #150 loss: 0.06868481636047363\n",
      "Epoch #100 iteration #200 loss: 0.08779819309711456\n",
      "Epoch #100 iteration #250 loss: 0.0679374411702156\n",
      "Epoch #100 iteration #300 loss: 0.0978548675775528\n",
      "Took 1.781 minutes for epoch# 100 to train\n",
      "Epoch #100 Train loss: 0.07187757811819513\n",
      "Epoch #100 Validation Precision: 0.5138888888888888\n",
      "Epoch #101 iteration #50 loss: 0.09071332961320877\n",
      "Epoch #101 iteration #100 loss: 0.07518191635608673\n",
      "Epoch #101 iteration #150 loss: 0.05928529053926468\n",
      "Epoch #101 iteration #200 loss: 0.06857534497976303\n",
      "Epoch #101 iteration #250 loss: 0.05873577669262886\n",
      "Epoch #101 iteration #300 loss: 0.09327269345521927\n",
      "Took 1.927 minutes for epoch# 101 to train\n",
      "Epoch #101 Train loss: 0.0713077516008455\n",
      "Epoch #101 Validation Precision: 0.5\n",
      "Epoch #102 iteration #50 loss: 0.0730939656496048\n",
      "Epoch #102 iteration #100 loss: 0.06819023191928864\n",
      "Epoch #102 iteration #150 loss: 0.06954365223646164\n",
      "Epoch #102 iteration #200 loss: 0.06435298919677734\n",
      "Epoch #102 iteration #250 loss: 0.07838379591703415\n",
      "Epoch #102 iteration #300 loss: 0.06408873200416565\n",
      "Took 1.779 minutes for epoch# 102 to train\n",
      "Epoch #102 Train loss: 0.06970285330540858\n",
      "Epoch #102 Validation Precision: 0.27588383838383834\n",
      "Epoch #103 iteration #50 loss: 0.08124662190675735\n",
      "Epoch #103 iteration #100 loss: 0.06181052327156067\n",
      "Epoch #103 iteration #150 loss: 0.05425246059894562\n",
      "Epoch #103 iteration #200 loss: 0.08378925174474716\n",
      "Epoch #103 iteration #250 loss: 0.07466908544301987\n",
      "Epoch #103 iteration #300 loss: 0.07649659365415573\n",
      "Took 1.779 minutes for epoch# 103 to train\n",
      "Epoch #103 Train loss: 0.06774679788698752\n",
      "Epoch #103 Validation Precision: 0.27604166666666663\n",
      "Epoch #104 iteration #50 loss: 0.09736358374357224\n",
      "Epoch #104 iteration #100 loss: 0.08832680433988571\n",
      "Epoch #104 iteration #150 loss: 0.061577774584293365\n",
      "Epoch #104 iteration #200 loss: 0.05553305521607399\n",
      "Epoch #104 iteration #250 loss: 0.07289091497659683\n",
      "Epoch #104 iteration #300 loss: 0.0601874440908432\n",
      "Took 1.777 minutes for epoch# 104 to train\n",
      "Epoch #104 Train loss: 0.06830561994455564\n",
      "Epoch #104 Validation Precision: 0.38888888888888884\n",
      "Epoch #105 iteration #50 loss: 0.09149238467216492\n",
      "Epoch #105 iteration #100 loss: 0.11978156864643097\n",
      "Epoch #105 iteration #150 loss: 0.06818673759698868\n",
      "Epoch #105 iteration #200 loss: 0.07507877796888351\n",
      "Epoch #105 iteration #250 loss: 0.05261080339550972\n",
      "Epoch #105 iteration #300 loss: 0.0736306682229042\n",
      "Took 1.775 minutes for epoch# 105 to train\n",
      "Epoch #105 Train loss: 0.07079235805819432\n",
      "Epoch #105 Validation Precision: 0.3194444444444444\n",
      "Epoch #106 iteration #50 loss: 0.0855802670121193\n",
      "Epoch #106 iteration #100 loss: 0.061860792338848114\n",
      "Epoch #106 iteration #150 loss: 0.06985405832529068\n",
      "Epoch #106 iteration #200 loss: 0.07520993053913116\n",
      "Epoch #106 iteration #250 loss: 0.06347411870956421\n",
      "Epoch #106 iteration #300 loss: 0.0653761625289917\n",
      "Took 1.778 minutes for epoch# 106 to train\n",
      "Epoch #106 Train loss: 0.0678286416671024\n",
      "Epoch #106 Validation Precision: 0.5069444444444444\n",
      "Epoch #107 iteration #50 loss: 0.10135577619075775\n",
      "Epoch #107 iteration #100 loss: 0.05698752775788307\n",
      "Epoch #107 iteration #150 loss: 0.061504051089286804\n",
      "Epoch #107 iteration #200 loss: 0.07883370667695999\n",
      "Epoch #107 iteration #250 loss: 0.0705813616514206\n",
      "Epoch #107 iteration #300 loss: 0.06387155503034592\n",
      "Took 1.778 minutes for epoch# 107 to train\n",
      "Epoch #107 Train loss: 0.06606343833920665\n",
      "Epoch #107 Validation Precision: 0.4821428571428571\n",
      "Epoch #108 iteration #50 loss: 0.08207754045724869\n",
      "Epoch #108 iteration #100 loss: 0.05525106191635132\n",
      "Epoch #108 iteration #150 loss: 0.05245637893676758\n",
      "Epoch #108 iteration #200 loss: 0.07851866632699966\n",
      "Epoch #108 iteration #250 loss: 0.08211959898471832\n",
      "Epoch #108 iteration #300 loss: 0.06886875629425049\n",
      "Took 1.782 minutes for epoch# 108 to train\n",
      "Epoch #108 Train loss: 0.06699961628048466\n",
      "Epoch #108 Validation Precision: 0.4944444444444444\n",
      "Epoch #109 iteration #50 loss: 0.07761894911527634\n",
      "Epoch #109 iteration #100 loss: 0.07598092406988144\n",
      "Epoch #109 iteration #150 loss: 0.08092768490314484\n",
      "Epoch #109 iteration #200 loss: 0.058387886732816696\n",
      "Epoch #109 iteration #250 loss: 0.05279664695262909\n",
      "Epoch #109 iteration #300 loss: 0.08026827871799469\n",
      "Took 1.779 minutes for epoch# 109 to train\n",
      "Epoch #109 Train loss: 0.06747316803114536\n",
      "Epoch #109 Validation Precision: 0.4583333333333333\n",
      "Epoch #110 iteration #50 loss: 0.09996629506349564\n",
      "Epoch #110 iteration #100 loss: 0.07259535044431686\n",
      "Epoch #110 iteration #150 loss: 0.06201023608446121\n",
      "Epoch #110 iteration #200 loss: 0.06789343059062958\n",
      "Epoch #110 iteration #250 loss: 0.06451335549354553\n",
      "Epoch #110 iteration #300 loss: 0.07778109610080719\n",
      "Took 1.781 minutes for epoch# 110 to train\n",
      "Epoch #110 Train loss: 0.06561332192415228\n",
      "Epoch #110 Validation Precision: 0.5208333333333333\n",
      "Epoch #111 iteration #50 loss: 0.08228907734155655\n",
      "Epoch #111 iteration #100 loss: 0.07053611427545547\n",
      "Epoch #111 iteration #150 loss: 0.07227680832147598\n",
      "Epoch #111 iteration #200 loss: 0.06640228629112244\n",
      "Epoch #111 iteration #250 loss: 0.052888624370098114\n",
      "Epoch #111 iteration #300 loss: 0.06813844293355942\n",
      "Took 1.777 minutes for epoch# 111 to train\n",
      "Epoch #111 Train loss: 0.06603393109085468\n",
      "Epoch #111 Validation Precision: 0.5\n",
      "Epoch #112 iteration #50 loss: 0.06803616136312485\n",
      "Epoch #112 iteration #100 loss: 0.07931126654148102\n",
      "Epoch #112 iteration #150 loss: 0.057177793234586716\n",
      "Epoch #112 iteration #200 loss: 0.054757289588451385\n",
      "Epoch #112 iteration #250 loss: 0.07050319015979767\n",
      "Epoch #112 iteration #300 loss: 0.08321935683488846\n",
      "Took 1.778 minutes for epoch# 112 to train\n",
      "Epoch #112 Train loss: 0.06652941750601316\n",
      "Epoch #112 Validation Precision: 0.5\n",
      "Epoch #113 iteration #50 loss: 0.12315716594457626\n",
      "Epoch #113 iteration #100 loss: 0.06778574734926224\n",
      "Epoch #113 iteration #150 loss: 0.04731152206659317\n",
      "Epoch #113 iteration #200 loss: 0.054405562579631805\n",
      "Epoch #113 iteration #250 loss: 0.06443395465612411\n",
      "Epoch #113 iteration #300 loss: 0.061359744518995285\n",
      "Took 1.778 minutes for epoch# 113 to train\n",
      "Epoch #113 Train loss: 0.06543045409787925\n",
      "Epoch #113 Validation Precision: 0.6319444444444444\n",
      "Epoch #114 iteration #50 loss: 0.08061610907316208\n",
      "Epoch #114 iteration #100 loss: 0.06806151568889618\n",
      "Epoch #114 iteration #150 loss: 0.049720969051122665\n",
      "Epoch #114 iteration #200 loss: 0.06297899037599564\n",
      "Epoch #114 iteration #250 loss: 0.05510665476322174\n",
      "Epoch #114 iteration #300 loss: 0.08006313443183899\n",
      "Took 1.781 minutes for epoch# 114 to train\n",
      "Epoch #114 Train loss: 0.06530075879075015\n",
      "Epoch #114 Validation Precision: 0.4666666666666666\n",
      "Epoch #115 iteration #50 loss: 0.07255245000123978\n",
      "Epoch #115 iteration #100 loss: 0.07051405310630798\n",
      "Epoch #115 iteration #150 loss: 0.048505961894989014\n",
      "Epoch #115 iteration #200 loss: 0.108051598072052\n",
      "Epoch #115 iteration #250 loss: 0.05838847532868385\n",
      "Epoch #115 iteration #300 loss: 0.055856067687273026\n",
      "Took 1.780 minutes for epoch# 115 to train\n",
      "Epoch #115 Train loss: 0.06406592769930378\n",
      "Epoch #115 Validation Precision: 0.45833333333333337\n",
      "Epoch #116 iteration #50 loss: 0.0590154267847538\n",
      "Epoch #116 iteration #100 loss: 0.06968692690134048\n",
      "Epoch #116 iteration #150 loss: 0.07142046093940735\n",
      "Epoch #116 iteration #200 loss: 0.06888151913881302\n",
      "Epoch #116 iteration #250 loss: 0.05054798349738121\n",
      "Epoch #116 iteration #300 loss: 0.07888369262218475\n",
      "Took 1.780 minutes for epoch# 116 to train\n",
      "Epoch #116 Train loss: 0.06469222567736721\n",
      "Epoch #116 Validation Precision: 0.5\n",
      "Epoch #117 iteration #50 loss: 0.07846813648939133\n",
      "Epoch #117 iteration #100 loss: 0.08152558654546738\n",
      "Epoch #117 iteration #150 loss: 0.06162499636411667\n",
      "Epoch #117 iteration #200 loss: 0.06842271983623505\n",
      "Epoch #117 iteration #250 loss: 0.08139725774526596\n",
      "Epoch #117 iteration #300 loss: 0.059323642402887344\n",
      "Took 1.776 minutes for epoch# 117 to train\n",
      "Epoch #117 Train loss: 0.06352589989845188\n",
      "Epoch #117 Validation Precision: 0.4916666666666667\n",
      "Epoch #118 iteration #50 loss: 0.07334867119789124\n",
      "Epoch #118 iteration #100 loss: 0.0646962970495224\n",
      "Epoch #118 iteration #150 loss: 0.043194614350795746\n",
      "Epoch #118 iteration #200 loss: 0.05815243721008301\n",
      "Epoch #118 iteration #250 loss: 0.053695522248744965\n",
      "Epoch #118 iteration #300 loss: 0.07833073288202286\n",
      "Took 1.779 minutes for epoch# 118 to train\n",
      "Epoch #118 Train loss: 0.06342999535636643\n",
      "Epoch #118 Validation Precision: 0.4944444444444444\n",
      "Epoch #119 iteration #50 loss: 0.07823647558689117\n",
      "Epoch #119 iteration #100 loss: 0.06058087199926376\n",
      "Epoch #119 iteration #150 loss: 0.05092928186058998\n",
      "Epoch #119 iteration #200 loss: 0.08681348711252213\n",
      "Epoch #119 iteration #250 loss: 0.07144176959991455\n",
      "Epoch #119 iteration #300 loss: 0.0670272633433342\n",
      "Took 1.778 minutes for epoch# 119 to train\n",
      "Epoch #119 Train loss: 0.06381939146190117\n",
      "Epoch #119 Validation Precision: 0.4666666666666666\n",
      "Epoch #120 iteration #50 loss: 0.07128190249204636\n",
      "Epoch #120 iteration #100 loss: 0.09869349747896194\n",
      "Epoch #120 iteration #150 loss: 0.0442185178399086\n",
      "Epoch #120 iteration #200 loss: 0.05326481908559799\n",
      "Epoch #120 iteration #250 loss: 0.05139235779643059\n",
      "Epoch #120 iteration #300 loss: 0.05845089629292488\n",
      "Took 1.780 minutes for epoch# 120 to train\n",
      "Epoch #120 Train loss: 0.06484843193529508\n",
      "Epoch #120 Validation Precision: 0.4722222222222222\n",
      "Epoch #121 iteration #50 loss: 0.09707539528608322\n",
      "Epoch #121 iteration #100 loss: 0.043524015694856644\n",
      "Epoch #121 iteration #150 loss: 0.04248066991567612\n",
      "Epoch #121 iteration #200 loss: 0.06702922284603119\n",
      "Epoch #121 iteration #250 loss: 0.05037190392613411\n",
      "Epoch #121 iteration #300 loss: 0.05643024295568466\n",
      "Took 1.780 minutes for epoch# 121 to train\n",
      "Epoch #121 Train loss: 0.062260711493973546\n",
      "Epoch #121 Validation Precision: 0.5624999999999999\n",
      "Epoch #122 iteration #50 loss: 0.08028148114681244\n",
      "Epoch #122 iteration #100 loss: 0.05627884343266487\n",
      "Epoch #122 iteration #150 loss: 0.054269369691610336\n",
      "Epoch #122 iteration #200 loss: 0.07655450701713562\n",
      "Epoch #122 iteration #250 loss: 0.07457676529884338\n",
      "Epoch #122 iteration #300 loss: 0.09450219571590424\n",
      "Took 1.777 minutes for epoch# 122 to train\n",
      "Epoch #122 Train loss: 0.06454026903240727\n",
      "Epoch #122 Validation Precision: 0.45833333333333337\n",
      "Epoch #123 iteration #50 loss: 0.078645259141922\n",
      "Epoch #123 iteration #100 loss: 0.06679097563028336\n",
      "Epoch #123 iteration #150 loss: 0.04522564262151718\n",
      "Epoch #123 iteration #200 loss: 0.07204888761043549\n",
      "Epoch #123 iteration #250 loss: 0.05479899421334267\n",
      "Epoch #123 iteration #300 loss: 0.05629698187112808\n",
      "Took 1.780 minutes for epoch# 123 to train\n",
      "Epoch #123 Train loss: 0.0630966246796724\n",
      "Epoch #123 Validation Precision: 0.6388888888888888\n",
      "Epoch #124 iteration #50 loss: 0.0793972909450531\n",
      "Epoch #124 iteration #100 loss: 0.06702043861150742\n",
      "Epoch #124 iteration #150 loss: 0.055887620896101\n",
      "Epoch #124 iteration #200 loss: 0.05000082403421402\n",
      "Epoch #124 iteration #250 loss: 0.06898090243339539\n",
      "Epoch #124 iteration #300 loss: 0.04513553902506828\n",
      "Took 1.784 minutes for epoch# 124 to train\n",
      "Epoch #124 Train loss: 0.06101580196991563\n",
      "Epoch #124 Validation Precision: 0.7291666666666666\n",
      "Epoch #125 iteration #50 loss: 0.04706808179616928\n",
      "Epoch #125 iteration #100 loss: 0.057028260082006454\n",
      "Epoch #125 iteration #150 loss: 0.06330599635839462\n",
      "Epoch #125 iteration #200 loss: 0.07544347643852234\n",
      "Epoch #125 iteration #250 loss: 0.04924837127327919\n",
      "Epoch #125 iteration #300 loss: 0.06926269084215164\n",
      "Took 1.783 minutes for epoch# 125 to train\n",
      "Epoch #125 Train loss: 0.0595065451059968\n",
      "Epoch #125 Validation Precision: 0.6875\n",
      "Epoch #126 iteration #50 loss: 0.10030379891395569\n",
      "Epoch #126 iteration #100 loss: 0.07557711005210876\n",
      "Epoch #126 iteration #150 loss: 0.0509888231754303\n",
      "Epoch #126 iteration #200 loss: 0.06787334382534027\n",
      "Epoch #126 iteration #250 loss: 0.06897827982902527\n",
      "Epoch #126 iteration #300 loss: 0.06926742941141129\n",
      "Took 1.784 minutes for epoch# 126 to train\n",
      "Epoch #126 Train loss: 0.06029362254179059\n",
      "Epoch #126 Validation Precision: 0.5958333333333333\n",
      "Epoch #127 iteration #50 loss: 0.0795745700597763\n",
      "Epoch #127 iteration #100 loss: 0.08189962059259415\n",
      "Epoch #127 iteration #150 loss: 0.055929265916347504\n",
      "Epoch #127 iteration #200 loss: 0.07165170460939407\n",
      "Epoch #127 iteration #250 loss: 0.04793231189250946\n",
      "Epoch #127 iteration #300 loss: 0.05874781683087349\n",
      "Took 1.779 minutes for epoch# 127 to train\n",
      "Epoch #127 Train loss: 0.05978032893453462\n",
      "Epoch #127 Validation Precision: 0.5277777777777778\n",
      "Epoch #128 iteration #50 loss: 0.09146783500909805\n",
      "Epoch #128 iteration #100 loss: 0.06888248026371002\n",
      "Epoch #128 iteration #150 loss: 0.043740905821323395\n",
      "Epoch #128 iteration #200 loss: 0.08378060907125473\n",
      "Epoch #128 iteration #250 loss: 0.061749041080474854\n",
      "Epoch #128 iteration #300 loss: 0.0643974170088768\n",
      "Took 1.780 minutes for epoch# 128 to train\n",
      "Epoch #128 Train loss: 0.05930013038838903\n",
      "Epoch #128 Validation Precision: 0.4930555555555556\n",
      "Epoch #129 iteration #50 loss: 0.08056947588920593\n",
      "Epoch #129 iteration #100 loss: 0.06832581758499146\n",
      "Epoch #129 iteration #150 loss: 0.04811263456940651\n",
      "Epoch #129 iteration #200 loss: 0.04873206466436386\n",
      "Epoch #129 iteration #250 loss: 0.06830701231956482\n",
      "Epoch #129 iteration #300 loss: 0.07009514421224594\n",
      "Took 1.782 minutes for epoch# 129 to train\n",
      "Epoch #129 Train loss: 0.062103963194367215\n",
      "Epoch #129 Validation Precision: 0.5729166666666666\n",
      "Epoch #130 iteration #50 loss: 0.09986236691474915\n",
      "Epoch #130 iteration #100 loss: 0.0745520368218422\n",
      "Epoch #130 iteration #150 loss: 0.06111285090446472\n",
      "Epoch #130 iteration #200 loss: 0.04232591390609741\n",
      "Epoch #130 iteration #250 loss: 0.04237150773406029\n",
      "Epoch #130 iteration #300 loss: 0.04440442845225334\n",
      "Took 1.778 minutes for epoch# 130 to train\n",
      "Epoch #130 Train loss: 0.06057282200512978\n",
      "Epoch #130 Validation Precision: 0.5260416666666666\n",
      "Epoch #131 iteration #50 loss: 0.0742754116654396\n",
      "Epoch #131 iteration #100 loss: 0.0586877278983593\n",
      "Epoch #131 iteration #150 loss: 0.05019598454236984\n",
      "Epoch #131 iteration #200 loss: 0.055035728961229324\n",
      "Epoch #131 iteration #250 loss: 0.06127152219414711\n",
      "Epoch #131 iteration #300 loss: 0.06898842751979828\n",
      "Took 1.778 minutes for epoch# 131 to train\n",
      "Epoch #131 Train loss: 0.05964773292772663\n",
      "Epoch #131 Validation Precision: 0.4444444444444444\n",
      "Epoch #132 iteration #50 loss: 0.05736522004008293\n",
      "Epoch #132 iteration #100 loss: 0.09114716947078705\n",
      "Epoch #132 iteration #150 loss: 0.07530543208122253\n",
      "Epoch #132 iteration #200 loss: 0.05583469942212105\n",
      "Epoch #132 iteration #250 loss: 0.07234726846218109\n",
      "Epoch #132 iteration #300 loss: 0.06645172089338303\n",
      "Took 1.782 minutes for epoch# 132 to train\n",
      "Epoch #132 Train loss: 0.060139390878761426\n",
      "Epoch #132 Validation Precision: 0.5277777777777778\n",
      "Epoch #133 iteration #50 loss: 0.051533471792936325\n",
      "Epoch #133 iteration #100 loss: 0.05340897664427757\n",
      "Epoch #133 iteration #150 loss: 0.030139124020934105\n",
      "Epoch #133 iteration #200 loss: 0.07077381759881973\n",
      "Epoch #133 iteration #250 loss: 0.05880626663565636\n",
      "Epoch #133 iteration #300 loss: 0.05850536376237869\n",
      "Took 1.779 minutes for epoch# 133 to train\n",
      "Epoch #133 Train loss: 0.05898548576097267\n",
      "Epoch #133 Validation Precision: 0.5624999999999999\n",
      "Epoch #134 iteration #50 loss: 0.0735466256737709\n",
      "Epoch #134 iteration #100 loss: 0.05532308667898178\n",
      "Epoch #134 iteration #150 loss: 0.049372732639312744\n",
      "Epoch #134 iteration #200 loss: 0.05951831489801407\n",
      "Epoch #134 iteration #250 loss: 0.06875607371330261\n",
      "Epoch #134 iteration #300 loss: 0.04717309772968292\n",
      "Took 1.782 minutes for epoch# 134 to train\n",
      "Epoch #134 Train loss: 0.05772623182155001\n",
      "Epoch #134 Validation Precision: 0.48611111111111105\n",
      "Epoch #135 iteration #50 loss: 0.06257110834121704\n",
      "Epoch #135 iteration #100 loss: 0.05572342127561569\n",
      "Epoch #135 iteration #150 loss: 0.062197282910346985\n",
      "Epoch #135 iteration #200 loss: 0.05236998200416565\n",
      "Epoch #135 iteration #250 loss: 0.0740448459982872\n",
      "Epoch #135 iteration #300 loss: 0.06649015843868256\n",
      "Took 1.782 minutes for epoch# 135 to train\n",
      "Epoch #135 Train loss: 0.05795766850217031\n",
      "Epoch #135 Validation Precision: 0.5277777777777778\n",
      "Epoch #136 iteration #50 loss: 0.054352063685655594\n",
      "Epoch #136 iteration #100 loss: 0.04904827103018761\n",
      "Epoch #136 iteration #150 loss: 0.08395636081695557\n",
      "Epoch #136 iteration #200 loss: 0.05121549591422081\n",
      "Epoch #136 iteration #250 loss: 0.08373282104730606\n",
      "Epoch #136 iteration #300 loss: 0.05690585821866989\n",
      "Took 1.780 minutes for epoch# 136 to train\n",
      "Epoch #136 Train loss: 0.05745122218743349\n",
      "Epoch #136 Validation Precision: 0.26388888888888884\n",
      "Epoch #137 iteration #50 loss: 0.050162333995103836\n",
      "Epoch #137 iteration #100 loss: 0.05682230740785599\n",
      "Epoch #137 iteration #150 loss: 0.06924443691968918\n",
      "Epoch #137 iteration #200 loss: 0.06730902194976807\n",
      "Epoch #137 iteration #250 loss: 0.04237912595272064\n",
      "Epoch #137 iteration #300 loss: 0.041431404650211334\n",
      "Took 1.782 minutes for epoch# 137 to train\n",
      "Epoch #137 Train loss: 0.05666070498335056\n",
      "Epoch #137 Validation Precision: 0.4305555555555556\n",
      "Epoch #138 iteration #50 loss: 0.07160155475139618\n",
      "Epoch #138 iteration #100 loss: 0.09077007323503494\n",
      "Epoch #138 iteration #150 loss: 0.05205225199460983\n",
      "Epoch #138 iteration #200 loss: 0.06622055917978287\n",
      "Epoch #138 iteration #250 loss: 0.0702609047293663\n",
      "Epoch #138 iteration #300 loss: 0.05322987958788872\n",
      "Took 1.779 minutes for epoch# 138 to train\n",
      "Epoch #138 Train loss: 0.05824538548357594\n",
      "Epoch #138 Validation Precision: 0.4666666666666667\n",
      "Epoch #139 iteration #50 loss: 0.0791822001338005\n",
      "Epoch #139 iteration #100 loss: 0.05343923717737198\n",
      "Epoch #139 iteration #150 loss: 0.06869205832481384\n",
      "Epoch #139 iteration #200 loss: 0.05152217298746109\n",
      "Epoch #139 iteration #250 loss: 0.07206996530294418\n",
      "Epoch #139 iteration #300 loss: 0.03963929042220116\n",
      "Took 1.779 minutes for epoch# 139 to train\n",
      "Epoch #139 Train loss: 0.05561465121662387\n",
      "Epoch #139 Validation Precision: 0.5\n",
      "Epoch #140 iteration #50 loss: 0.09524387866258621\n",
      "Epoch #140 iteration #100 loss: 0.05468076094985008\n",
      "Epoch #140 iteration #150 loss: 0.05666924640536308\n",
      "Epoch #140 iteration #200 loss: 0.06420354545116425\n",
      "Epoch #140 iteration #250 loss: 0.044095948338508606\n",
      "Epoch #140 iteration #300 loss: 0.0406794436275959\n",
      "Took 1.781 minutes for epoch# 140 to train\n",
      "Epoch #140 Train loss: 0.05659542287078997\n",
      "Epoch #140 Validation Precision: 0.6666666666666666\n",
      "Epoch #141 iteration #50 loss: 0.0832025483250618\n",
      "Epoch #141 iteration #100 loss: 0.055050093680620193\n",
      "Epoch #141 iteration #150 loss: 0.04154844209551811\n",
      "Epoch #141 iteration #200 loss: 0.0476096086204052\n",
      "Epoch #141 iteration #250 loss: 0.05463405326008797\n",
      "Epoch #141 iteration #300 loss: 0.05858507379889488\n",
      "Took 1.786 minutes for epoch# 141 to train\n",
      "Epoch #141 Train loss: 0.05582702834493457\n",
      "Epoch #141 Validation Precision: 0.41666666666666663\n",
      "Epoch #142 iteration #50 loss: 0.04707549139857292\n",
      "Epoch #142 iteration #100 loss: 0.06526236981153488\n",
      "Epoch #142 iteration #150 loss: 0.054500021040439606\n",
      "Epoch #142 iteration #200 loss: 0.06280536949634552\n",
      "Epoch #142 iteration #250 loss: 0.07286190241575241\n",
      "Epoch #142 iteration #300 loss: 0.046289246529340744\n",
      "Took 1.781 minutes for epoch# 142 to train\n",
      "Epoch #142 Train loss: 0.0558907705394981\n",
      "Epoch #142 Validation Precision: 0.29166666666666663\n",
      "Epoch #143 iteration #50 loss: 0.07025256752967834\n",
      "Epoch #143 iteration #100 loss: 0.05983804911375046\n",
      "Epoch #143 iteration #150 loss: 0.04248226433992386\n",
      "Epoch #143 iteration #200 loss: 0.05564536899328232\n",
      "Epoch #143 iteration #250 loss: 0.058185819536447525\n",
      "Epoch #143 iteration #300 loss: 0.056657154113054276\n",
      "Took 1.785 minutes for epoch# 143 to train\n",
      "Epoch #143 Train loss: 0.05564921317048944\n",
      "Epoch #143 Validation Precision: 0.3333333333333333\n",
      "Epoch #144 iteration #50 loss: 0.05959697067737579\n",
      "Epoch #144 iteration #100 loss: 0.04328084737062454\n",
      "Epoch #144 iteration #150 loss: 0.04830484092235565\n",
      "Epoch #144 iteration #200 loss: 0.07093000411987305\n",
      "Epoch #144 iteration #250 loss: 0.058932702988386154\n",
      "Epoch #144 iteration #300 loss: 0.055062633007764816\n",
      "Took 1.782 minutes for epoch# 144 to train\n",
      "Epoch #144 Train loss: 0.053902098610519596\n",
      "Epoch #144 Validation Precision: 0.4583333333333333\n",
      "Epoch #145 iteration #50 loss: 0.06519600749015808\n",
      "Epoch #145 iteration #100 loss: 0.060441892594099045\n",
      "Epoch #145 iteration #150 loss: 0.04447868838906288\n",
      "Epoch #145 iteration #200 loss: 0.046756595373153687\n",
      "Epoch #145 iteration #250 loss: 0.08513589203357697\n",
      "Epoch #145 iteration #300 loss: 0.04255259409546852\n",
      "Took 1.782 minutes for epoch# 145 to train\n",
      "Epoch #145 Train loss: 0.0548155482333058\n",
      "Epoch #145 Validation Precision: 0.37499999999999994\n",
      "Epoch #146 iteration #50 loss: 0.04625106230378151\n",
      "Epoch #146 iteration #100 loss: 0.06141021475195885\n",
      "Epoch #146 iteration #150 loss: 0.04993864893913269\n",
      "Epoch #146 iteration #200 loss: 0.05056199058890343\n",
      "Epoch #146 iteration #250 loss: 0.045014891773462296\n",
      "Epoch #146 iteration #300 loss: 0.03293822705745697\n",
      "Took 1.783 minutes for epoch# 146 to train\n",
      "Epoch #146 Train loss: 0.054704428220597595\n",
      "Epoch #146 Validation Precision: 0.47916666666666663\n",
      "Epoch #147 iteration #50 loss: 0.08394839614629745\n",
      "Epoch #147 iteration #100 loss: 0.05246087163686752\n",
      "Epoch #147 iteration #150 loss: 0.033467233180999756\n",
      "Epoch #147 iteration #200 loss: 0.07079605013132095\n",
      "Epoch #147 iteration #250 loss: 0.049758777022361755\n",
      "Epoch #147 iteration #300 loss: 0.045760516077280045\n",
      "Took 1.779 minutes for epoch# 147 to train\n",
      "Epoch #147 Train loss: 0.055727732714074545\n",
      "Epoch #147 Validation Precision: 0.5555555555555555\n",
      "Epoch #148 iteration #50 loss: 0.07095051556825638\n",
      "Epoch #148 iteration #100 loss: 0.06145425885915756\n",
      "Epoch #148 iteration #150 loss: 0.029166758060455322\n",
      "Epoch #148 iteration #200 loss: 0.08056473731994629\n",
      "Epoch #148 iteration #250 loss: 0.04947070777416229\n",
      "Epoch #148 iteration #300 loss: 0.08814401179552078\n",
      "Took 1.779 minutes for epoch# 148 to train\n",
      "Epoch #148 Train loss: 0.05476815807513702\n",
      "Epoch #148 Validation Precision: 0.5277777777777777\n",
      "Epoch #149 iteration #50 loss: 0.06161342188715935\n",
      "Epoch #149 iteration #100 loss: 0.06457603722810745\n",
      "Epoch #149 iteration #150 loss: 0.04895520210266113\n",
      "Epoch #149 iteration #200 loss: 0.05172547698020935\n",
      "Epoch #149 iteration #250 loss: 0.0649590790271759\n",
      "Epoch #149 iteration #300 loss: 0.08302202075719833\n",
      "Took 1.782 minutes for epoch# 149 to train\n",
      "Epoch #149 Train loss: 0.05326594794407869\n",
      "Epoch #149 Validation Precision: 0.5\n",
      "Epoch #150 iteration #50 loss: 0.06192348152399063\n",
      "Epoch #150 iteration #100 loss: 0.06766487658023834\n",
      "Epoch #150 iteration #150 loss: 0.04859840124845505\n",
      "Epoch #150 iteration #200 loss: 0.06369926780462265\n",
      "Epoch #150 iteration #250 loss: 0.039728451520204544\n",
      "Epoch #150 iteration #300 loss: 0.050196509808301926\n",
      "Took 1.782 minutes for epoch# 150 to train\n",
      "Epoch #150 Train loss: 0.05294077382542384\n",
      "Epoch #150 Validation Precision: 0.5416666666666666\n",
      "Epoch #151 iteration #50 loss: 0.055857378989458084\n",
      "Epoch #151 iteration #100 loss: 0.06872078031301498\n",
      "Epoch #151 iteration #150 loss: 0.03601270169019699\n",
      "Epoch #151 iteration #200 loss: 0.05993025377392769\n",
      "Epoch #151 iteration #250 loss: 0.0705530047416687\n",
      "Epoch #151 iteration #300 loss: 0.046256374567747116\n",
      "Took 1.779 minutes for epoch# 151 to train\n",
      "Epoch #151 Train loss: 0.053664284465738975\n",
      "Epoch #151 Validation Precision: 0.7291666666666666\n",
      "Epoch #152 iteration #50 loss: 0.046468883752822876\n",
      "Epoch #152 iteration #100 loss: 0.07019370049238205\n",
      "Epoch #152 iteration #150 loss: 0.055198416113853455\n",
      "Epoch #152 iteration #200 loss: 0.052273228764534\n",
      "Epoch #152 iteration #250 loss: 0.05833958461880684\n",
      "Epoch #152 iteration #300 loss: 0.04061979055404663\n",
      "Took 1.780 minutes for epoch# 152 to train\n",
      "Epoch #152 Train loss: 0.053392290842170134\n",
      "Epoch #152 Validation Precision: 0.5\n",
      "Epoch #153 iteration #50 loss: 0.059026479721069336\n",
      "Epoch #153 iteration #100 loss: 0.06254762411117554\n",
      "Epoch #153 iteration #150 loss: 0.03395330533385277\n",
      "Epoch #153 iteration #200 loss: 0.040525373071432114\n",
      "Epoch #153 iteration #250 loss: 0.06376317143440247\n",
      "Epoch #153 iteration #300 loss: 0.04989703372120857\n",
      "Took 1.784 minutes for epoch# 153 to train\n",
      "Epoch #153 Train loss: 0.05293111510885259\n",
      "Epoch #153 Validation Precision: 0.47916666666666663\n",
      "Epoch #154 iteration #50 loss: 0.06648856401443481\n",
      "Epoch #154 iteration #100 loss: 0.05978673696517944\n",
      "Epoch #154 iteration #150 loss: 0.05779746547341347\n",
      "Epoch #154 iteration #200 loss: 0.05187692865729332\n",
      "Epoch #154 iteration #250 loss: 0.043393999338150024\n",
      "Epoch #154 iteration #300 loss: 0.045770853757858276\n",
      "Took 1.782 minutes for epoch# 154 to train\n",
      "Epoch #154 Train loss: 0.05163860857152404\n",
      "Epoch #154 Validation Precision: 0.5625\n",
      "Epoch #155 iteration #50 loss: 0.09475758671760559\n",
      "Epoch #155 iteration #100 loss: 0.04853593930602074\n",
      "Epoch #155 iteration #150 loss: 0.05189928784966469\n",
      "Epoch #155 iteration #200 loss: 0.045569926500320435\n",
      "Epoch #155 iteration #250 loss: 0.05962825566530228\n",
      "Epoch #155 iteration #300 loss: 0.057302478700876236\n",
      "Took 1.780 minutes for epoch# 155 to train\n",
      "Epoch #155 Train loss: 0.051211742630514964\n",
      "Epoch #155 Validation Precision: 0.3333333333333333\n",
      "Epoch #156 iteration #50 loss: 0.0757603645324707\n",
      "Epoch #156 iteration #100 loss: 0.04751328006386757\n",
      "Epoch #156 iteration #150 loss: 0.048234131187200546\n",
      "Epoch #156 iteration #200 loss: 0.0723281130194664\n",
      "Epoch #156 iteration #250 loss: 0.04710153862833977\n",
      "Epoch #156 iteration #300 loss: 0.04744963347911835\n",
      "Took 1.787 minutes for epoch# 156 to train\n",
      "Epoch #156 Train loss: 0.052129168493243366\n",
      "Epoch #156 Validation Precision: 0.6249999999999999\n",
      "Epoch #157 iteration #50 loss: 0.07279197871685028\n",
      "Epoch #157 iteration #100 loss: 0.05463647469878197\n",
      "Epoch #157 iteration #150 loss: 0.04142794385552406\n",
      "Epoch #157 iteration #200 loss: 0.04042649269104004\n",
      "Epoch #157 iteration #250 loss: 0.06605753302574158\n",
      "Epoch #157 iteration #300 loss: 0.04786774516105652\n",
      "Took 1.780 minutes for epoch# 157 to train\n",
      "Epoch #157 Train loss: 0.05099332033513257\n",
      "Epoch #157 Validation Precision: 0.44791666666666663\n",
      "Epoch #158 iteration #50 loss: 0.06650051474571228\n",
      "Epoch #158 iteration #100 loss: 0.053685545921325684\n",
      "Epoch #158 iteration #150 loss: 0.04544041305780411\n",
      "Epoch #158 iteration #200 loss: 0.06967855244874954\n",
      "Epoch #158 iteration #250 loss: 0.06310451775789261\n",
      "Epoch #158 iteration #300 loss: 0.06072200834751129\n",
      "Took 1.781 minutes for epoch# 158 to train\n",
      "Epoch #158 Train loss: 0.052037306332912966\n",
      "Epoch #158 Validation Precision: 0.6249999999999999\n",
      "Epoch #159 iteration #50 loss: 0.08460956811904907\n",
      "Epoch #159 iteration #100 loss: 0.05952516943216324\n",
      "Epoch #159 iteration #150 loss: 0.0454045794904232\n",
      "Epoch #159 iteration #200 loss: 0.06389708817005157\n",
      "Epoch #159 iteration #250 loss: 0.049750927835702896\n",
      "Epoch #159 iteration #300 loss: 0.0582897923886776\n",
      "Took 1.782 minutes for epoch# 159 to train\n",
      "Epoch #159 Train loss: 0.0516370362124573\n",
      "Epoch #159 Validation Precision: 0.6875\n",
      "Epoch #160 iteration #50 loss: 0.06536103785037994\n",
      "Epoch #160 iteration #100 loss: 0.04864758998155594\n",
      "Epoch #160 iteration #150 loss: 0.027061326429247856\n",
      "Epoch #160 iteration #200 loss: 0.07135903835296631\n",
      "Epoch #160 iteration #250 loss: 0.05114945024251938\n",
      "Epoch #160 iteration #300 loss: 0.03754961118102074\n",
      "Took 1.785 minutes for epoch# 160 to train\n",
      "Epoch #160 Train loss: 0.04985866617435255\n",
      "Epoch #160 Validation Precision: 0.6527777777777777\n",
      "Epoch #161 iteration #50 loss: 0.06644072383642197\n",
      "Epoch #161 iteration #100 loss: 0.08346366137266159\n",
      "Epoch #161 iteration #150 loss: 0.045862745493650436\n",
      "Epoch #161 iteration #200 loss: 0.06839283555746078\n",
      "Epoch #161 iteration #250 loss: 0.050108782947063446\n",
      "Epoch #161 iteration #300 loss: 0.0491049587726593\n",
      "Took 1.786 minutes for epoch# 161 to train\n",
      "Epoch #161 Train loss: 0.050770953142394624\n",
      "Epoch #161 Validation Precision: 0.6249999999999999\n",
      "Epoch #162 iteration #50 loss: 0.06972020864486694\n",
      "Epoch #162 iteration #100 loss: 0.06163572892546654\n",
      "Epoch #162 iteration #150 loss: 0.056105080991983414\n",
      "Epoch #162 iteration #200 loss: 0.06039537861943245\n",
      "Epoch #162 iteration #250 loss: 0.0372031144797802\n",
      "Epoch #162 iteration #300 loss: 0.04786225035786629\n",
      "Took 1.781 minutes for epoch# 162 to train\n",
      "Epoch #162 Train loss: 0.0503102600789414\n",
      "Epoch #162 Validation Precision: 0.5069444444444444\n",
      "Epoch #163 iteration #50 loss: 0.05827387049794197\n",
      "Epoch #163 iteration #100 loss: 0.037702322006225586\n",
      "Epoch #163 iteration #150 loss: 0.07099880278110504\n",
      "Epoch #163 iteration #200 loss: 0.03863057121634483\n",
      "Epoch #163 iteration #250 loss: 0.06058434769511223\n",
      "Epoch #163 iteration #300 loss: 0.04918836057186127\n",
      "Took 1.782 minutes for epoch# 163 to train\n",
      "Epoch #163 Train loss: 0.04960614974753788\n",
      "Epoch #163 Validation Precision: 0.6249999999999999\n",
      "Epoch #164 iteration #50 loss: 0.06951262056827545\n",
      "Epoch #164 iteration #100 loss: 0.04332475736737251\n",
      "Epoch #164 iteration #150 loss: 0.059395402669906616\n",
      "Epoch #164 iteration #200 loss: 0.06004086881875992\n",
      "Epoch #164 iteration #250 loss: 0.05023641511797905\n",
      "Epoch #164 iteration #300 loss: 0.055318161845207214\n",
      "Took 1.781 minutes for epoch# 164 to train\n",
      "Epoch #164 Train loss: 0.05089248657728044\n",
      "Epoch #164 Validation Precision: 0.8194444444444444\n",
      "Epoch #165 iteration #50 loss: 0.07415162771940231\n",
      "Epoch #165 iteration #100 loss: 0.04807424917817116\n",
      "Epoch #165 iteration #150 loss: 0.04098237305879593\n",
      "Epoch #165 iteration #200 loss: 0.05547285079956055\n",
      "Epoch #165 iteration #250 loss: 0.06893224269151688\n",
      "Epoch #165 iteration #300 loss: 0.030861958861351013\n",
      "Took 1.781 minutes for epoch# 165 to train\n",
      "Epoch #165 Train loss: 0.04975288718317946\n",
      "Epoch #165 Validation Precision: 0.6249999999999999\n",
      "Epoch #166 iteration #50 loss: 0.04908648133277893\n",
      "Epoch #166 iteration #100 loss: 0.05580540746450424\n",
      "Epoch #166 iteration #150 loss: 0.04201766848564148\n",
      "Epoch #166 iteration #200 loss: 0.0705658495426178\n",
      "Epoch #166 iteration #250 loss: 0.05193842947483063\n",
      "Epoch #166 iteration #300 loss: 0.031064379960298538\n",
      "Took 1.783 minutes for epoch# 166 to train\n",
      "Epoch #166 Train loss: 0.04834001534021436\n",
      "Epoch #166 Validation Precision: 0.611111111111111\n",
      "Epoch #167 iteration #50 loss: 0.058242689818143845\n",
      "Epoch #167 iteration #100 loss: 0.052242230623960495\n",
      "Epoch #167 iteration #150 loss: 0.034546785056591034\n",
      "Epoch #167 iteration #200 loss: 0.05991842597723007\n",
      "Epoch #167 iteration #250 loss: 0.040632303804159164\n",
      "Epoch #167 iteration #300 loss: 0.04716400057077408\n",
      "Took 1.784 minutes for epoch# 167 to train\n",
      "Epoch #167 Train loss: 0.04861989114672328\n",
      "Epoch #167 Validation Precision: 0.7499999999999999\n",
      "Epoch #168 iteration #50 loss: 0.07951445877552032\n",
      "Epoch #168 iteration #100 loss: 0.05495532974600792\n",
      "Epoch #168 iteration #150 loss: 0.03043515980243683\n",
      "Epoch #168 iteration #200 loss: 0.05867389589548111\n",
      "Epoch #168 iteration #250 loss: 0.04568826034665108\n",
      "Epoch #168 iteration #300 loss: 0.04686770588159561\n",
      "Took 1.781 minutes for epoch# 168 to train\n",
      "Epoch #168 Train loss: 0.049520390012707464\n",
      "Epoch #168 Validation Precision: 0.7499999999999999\n",
      "Epoch #169 iteration #50 loss: 0.04597190022468567\n",
      "Epoch #169 iteration #100 loss: 0.04811691492795944\n",
      "Epoch #169 iteration #150 loss: 0.03813519701361656\n",
      "Epoch #169 iteration #200 loss: 0.04780863597989082\n",
      "Epoch #169 iteration #250 loss: 0.04583060368895531\n",
      "Epoch #169 iteration #300 loss: 0.08818382024765015\n",
      "Took 1.782 minutes for epoch# 169 to train\n",
      "Epoch #169 Train loss: 0.04943431709081125\n",
      "Epoch #169 Validation Precision: 0.7499999999999999\n",
      "Epoch #170 iteration #50 loss: 0.05733291804790497\n",
      "Epoch #170 iteration #100 loss: 0.056840281933546066\n",
      "Epoch #170 iteration #150 loss: 0.04614289477467537\n",
      "Epoch #170 iteration #200 loss: 0.03839879855513573\n",
      "Epoch #170 iteration #250 loss: 0.04588044807314873\n",
      "Epoch #170 iteration #300 loss: 0.03402072191238403\n",
      "Took 1.783 minutes for epoch# 170 to train\n",
      "Epoch #170 Train loss: 0.047540251744720034\n",
      "Epoch #170 Validation Precision: 0.7499999999999999\n",
      "Epoch #171 iteration #50 loss: 0.04759813845157623\n",
      "Epoch #171 iteration #100 loss: 0.10816527903079987\n",
      "Epoch #171 iteration #150 loss: 0.05113346502184868\n",
      "Epoch #171 iteration #200 loss: 0.04638679325580597\n",
      "Epoch #171 iteration #250 loss: 0.033649127930402756\n",
      "Epoch #171 iteration #300 loss: 0.052984341979026794\n",
      "Took 1.786 minutes for epoch# 171 to train\n",
      "Epoch #171 Train loss: 0.04788060176472824\n",
      "Epoch #171 Validation Precision: 0.7083333333333333\n",
      "Epoch #172 iteration #50 loss: 0.06555583328008652\n",
      "Epoch #172 iteration #100 loss: 0.04615679010748863\n",
      "Epoch #172 iteration #150 loss: 0.03347722813487053\n",
      "Epoch #172 iteration #200 loss: 0.0379023402929306\n",
      "Epoch #172 iteration #250 loss: 0.05398488789796829\n",
      "Epoch #172 iteration #300 loss: 0.04749562591314316\n",
      "Took 1.782 minutes for epoch# 172 to train\n",
      "Epoch #172 Train loss: 0.047920301156596116\n",
      "Epoch #172 Validation Precision: 0.7499999999999999\n",
      "Epoch #173 iteration #50 loss: 0.06542477011680603\n",
      "Epoch #173 iteration #100 loss: 0.05326410382986069\n",
      "Epoch #173 iteration #150 loss: 0.04649427905678749\n",
      "Epoch #173 iteration #200 loss: 0.03718957304954529\n",
      "Epoch #173 iteration #250 loss: 0.04141783341765404\n",
      "Epoch #173 iteration #300 loss: 0.053127143532037735\n",
      "Took 1.783 minutes for epoch# 173 to train\n",
      "Epoch #173 Train loss: 0.04888773692222551\n",
      "Epoch #173 Validation Precision: 0.6388888888888888\n",
      "Epoch #174 iteration #50 loss: 0.06958752125501633\n",
      "Epoch #174 iteration #100 loss: 0.057513512670993805\n",
      "Epoch #174 iteration #150 loss: 0.04560791701078415\n",
      "Epoch #174 iteration #200 loss: 0.04371561110019684\n",
      "Epoch #174 iteration #250 loss: 0.04071855917572975\n",
      "Epoch #174 iteration #300 loss: 0.04063859209418297\n",
      "Took 1.784 minutes for epoch# 174 to train\n",
      "Epoch #174 Train loss: 0.048262625825233184\n",
      "Epoch #174 Validation Precision: 0.5833333333333333\n",
      "Epoch #175 iteration #50 loss: 0.05216846615076065\n",
      "Epoch #175 iteration #100 loss: 0.04588861018419266\n",
      "Epoch #175 iteration #150 loss: 0.04229532182216644\n",
      "Epoch #175 iteration #200 loss: 0.04328962415456772\n",
      "Epoch #175 iteration #250 loss: 0.05794202908873558\n",
      "Epoch #175 iteration #300 loss: 0.05686139315366745\n",
      "Took 1.783 minutes for epoch# 175 to train\n",
      "Epoch #175 Train loss: 0.04856594590446315\n",
      "Epoch #175 Validation Precision: 0.7083333333333333\n",
      "Epoch #176 iteration #50 loss: 0.04841471090912819\n",
      "Epoch #176 iteration #100 loss: 0.05493734031915665\n",
      "Epoch #176 iteration #150 loss: 0.06802932173013687\n",
      "Epoch #176 iteration #200 loss: 0.06341011822223663\n",
      "Epoch #176 iteration #250 loss: 0.04897996783256531\n",
      "Epoch #176 iteration #300 loss: 0.03634640574455261\n",
      "Took 1.782 minutes for epoch# 176 to train\n",
      "Epoch #176 Train loss: 0.04838234890037431\n",
      "Epoch #176 Validation Precision: 0.5416666666666666\n",
      "Epoch #177 iteration #50 loss: 0.047206562012434006\n",
      "Epoch #177 iteration #100 loss: 0.033627189695835114\n",
      "Epoch #177 iteration #150 loss: 0.05685129016637802\n",
      "Epoch #177 iteration #200 loss: 0.06931246072053909\n",
      "Epoch #177 iteration #250 loss: 0.05325146019458771\n",
      "Epoch #177 iteration #300 loss: 0.05887408182024956\n",
      "Took 1.782 minutes for epoch# 177 to train\n",
      "Epoch #177 Train loss: 0.04766575443462875\n",
      "Epoch #177 Validation Precision: 0.6458333333333333\n",
      "Epoch #178 iteration #50 loss: 0.07104992121458054\n",
      "Epoch #178 iteration #100 loss: 0.040654297918081284\n",
      "Epoch #178 iteration #150 loss: 0.044507332146167755\n",
      "Epoch #178 iteration #200 loss: 0.030667902901768684\n",
      "Epoch #178 iteration #250 loss: 0.04825872182846069\n",
      "Epoch #178 iteration #300 loss: 0.039076097309589386\n",
      "Took 1.787 minutes for epoch# 178 to train\n",
      "Epoch #178 Train loss: 0.04636544481110878\n",
      "Epoch #178 Validation Precision: 0.75\n",
      "Epoch #179 iteration #50 loss: 0.07320744544267654\n",
      "Epoch #179 iteration #100 loss: 0.06292688101530075\n",
      "Epoch #179 iteration #150 loss: 0.05035705491900444\n",
      "Epoch #179 iteration #200 loss: 0.05217784643173218\n",
      "Epoch #179 iteration #250 loss: 0.03914089873433113\n",
      "Epoch #179 iteration #300 loss: 0.053731583058834076\n",
      "Took 1.783 minutes for epoch# 179 to train\n",
      "Epoch #179 Train loss: 0.046933599813387565\n",
      "Epoch #179 Validation Precision: 0.6875\n",
      "Epoch #180 iteration #50 loss: 0.03895767778158188\n",
      "Epoch #180 iteration #100 loss: 0.045540932565927505\n",
      "Epoch #180 iteration #150 loss: 0.049202825874090195\n",
      "Epoch #180 iteration #200 loss: 0.05926303565502167\n",
      "Epoch #180 iteration #250 loss: 0.04469690099358559\n",
      "Epoch #180 iteration #300 loss: 0.039097048342227936\n",
      "Took 1.780 minutes for epoch# 180 to train\n",
      "Epoch #180 Train loss: 0.04738564641835789\n",
      "Epoch #180 Validation Precision: 0.7083333333333333\n",
      "Epoch #181 iteration #50 loss: 0.048493463546037674\n",
      "Epoch #181 iteration #100 loss: 0.045784804970026016\n",
      "Epoch #181 iteration #150 loss: 0.0476095974445343\n",
      "Epoch #181 iteration #200 loss: 0.05164942517876625\n",
      "Epoch #181 iteration #250 loss: 0.06413418054580688\n",
      "Epoch #181 iteration #300 loss: 0.042671993374824524\n",
      "Took 1.785 minutes for epoch# 181 to train\n",
      "Epoch #181 Train loss: 0.04733644225873435\n",
      "Epoch #181 Validation Precision: 0.8125\n",
      "Epoch #182 iteration #50 loss: 0.05241599678993225\n",
      "Epoch #182 iteration #100 loss: 0.06274528056383133\n",
      "Epoch #182 iteration #150 loss: 0.026621779426932335\n",
      "Epoch #182 iteration #200 loss: 0.03075323812663555\n",
      "Epoch #182 iteration #250 loss: 0.06450477987527847\n",
      "Epoch #182 iteration #300 loss: 0.04304036498069763\n",
      "Took 1.783 minutes for epoch# 182 to train\n",
      "Epoch #182 Train loss: 0.04687823265647659\n",
      "Epoch #182 Validation Precision: 0.4444444444444444\n",
      "Epoch #183 iteration #50 loss: 0.049865808337926865\n",
      "Epoch #183 iteration #100 loss: 0.06107379123568535\n",
      "Epoch #183 iteration #150 loss: 0.047599975019693375\n",
      "Epoch #183 iteration #200 loss: 0.05588094890117645\n",
      "Epoch #183 iteration #250 loss: 0.035361818969249725\n",
      "Epoch #183 iteration #300 loss: 0.0571647547185421\n",
      "Took 1.783 minutes for epoch# 183 to train\n",
      "Epoch #183 Train loss: 0.04689975915691601\n",
      "Epoch #183 Validation Precision: 0.4270833333333333\n",
      "Epoch #184 iteration #50 loss: 0.04921245574951172\n",
      "Epoch #184 iteration #100 loss: 0.04753374680876732\n",
      "Epoch #184 iteration #150 loss: 0.04131104797124863\n",
      "Epoch #184 iteration #200 loss: 0.06742145121097565\n",
      "Epoch #184 iteration #250 loss: 0.03710557892918587\n",
      "Epoch #184 iteration #300 loss: 0.04211292788386345\n",
      "Took 1.785 minutes for epoch# 184 to train\n",
      "Epoch #184 Train loss: 0.046066620285646655\n",
      "Epoch #184 Validation Precision: 0.49999999999999994\n",
      "Epoch #185 iteration #50 loss: 0.041899312287569046\n",
      "Epoch #185 iteration #100 loss: 0.052761711180210114\n",
      "Epoch #185 iteration #150 loss: 0.032090045511722565\n",
      "Epoch #185 iteration #200 loss: 0.042432479560375214\n",
      "Epoch #185 iteration #250 loss: 0.03514023497700691\n",
      "Epoch #185 iteration #300 loss: 0.07304910570383072\n",
      "Took 1.784 minutes for epoch# 185 to train\n",
      "Epoch #185 Train loss: 0.04605510492379276\n",
      "Epoch #185 Validation Precision: 0.7499999999999999\n",
      "Epoch #186 iteration #50 loss: 0.056692592799663544\n",
      "Epoch #186 iteration #100 loss: 0.026174120604991913\n",
      "Epoch #186 iteration #150 loss: 0.060521237552165985\n",
      "Epoch #186 iteration #200 loss: 0.045820124447345734\n",
      "Epoch #186 iteration #250 loss: 0.0402183011174202\n",
      "Epoch #186 iteration #300 loss: 0.04092808812856674\n",
      "Took 1.781 minutes for epoch# 186 to train\n",
      "Epoch #186 Train loss: 0.045355493417726114\n",
      "Epoch #186 Validation Precision: 0.7916666666666666\n",
      "Epoch #187 iteration #50 loss: 0.052726421505212784\n",
      "Epoch #187 iteration #100 loss: 0.03603491932153702\n",
      "Epoch #187 iteration #150 loss: 0.03869573399424553\n",
      "Epoch #187 iteration #200 loss: 0.03311878442764282\n",
      "Epoch #187 iteration #250 loss: 0.045190274715423584\n",
      "Epoch #187 iteration #300 loss: 0.04208718240261078\n",
      "Took 1.780 minutes for epoch# 187 to train\n",
      "Epoch #187 Train loss: 0.04582494134322191\n",
      "Epoch #187 Validation Precision: 0.7291666666666666\n",
      "Epoch #188 iteration #50 loss: 0.05156385526061058\n",
      "Epoch #188 iteration #100 loss: 0.08388478308916092\n",
      "Epoch #188 iteration #150 loss: 0.03818297013640404\n",
      "Epoch #188 iteration #200 loss: 0.05307350307703018\n",
      "Epoch #188 iteration #250 loss: 0.030362321063876152\n",
      "Epoch #188 iteration #300 loss: 0.03286762908101082\n",
      "Took 1.787 minutes for epoch# 188 to train\n",
      "Epoch #188 Train loss: 0.045231469112854354\n",
      "Epoch #188 Validation Precision: 0.6458333333333333\n",
      "Epoch #189 iteration #50 loss: 0.05867438763380051\n",
      "Epoch #189 iteration #100 loss: 0.05227293819189072\n",
      "Epoch #189 iteration #150 loss: 0.05795498192310333\n",
      "Epoch #189 iteration #200 loss: 0.05369858071208\n",
      "Epoch #189 iteration #250 loss: 0.04385770112276077\n",
      "Epoch #189 iteration #300 loss: 0.05084935203194618\n",
      "Took 1.781 minutes for epoch# 189 to train\n",
      "Epoch #189 Train loss: 0.045349290827289224\n",
      "Epoch #189 Validation Precision: 0.7083333333333333\n",
      "Epoch #190 iteration #50 loss: 0.053334061056375504\n",
      "Epoch #190 iteration #100 loss: 0.04986535385251045\n",
      "Epoch #190 iteration #150 loss: 0.04727537930011749\n",
      "Epoch #190 iteration #200 loss: 0.03232691437005997\n",
      "Epoch #190 iteration #250 loss: 0.04634513705968857\n",
      "Epoch #190 iteration #300 loss: 0.035563383251428604\n",
      "Took 1.782 minutes for epoch# 190 to train\n",
      "Epoch #190 Train loss: 0.0445035704327986\n",
      "Epoch #190 Validation Precision: 0.49999999999999994\n",
      "Epoch #191 iteration #50 loss: 0.07695158571004868\n",
      "Epoch #191 iteration #100 loss: 0.054984986782073975\n",
      "Epoch #191 iteration #150 loss: 0.04441550001502037\n",
      "Epoch #191 iteration #200 loss: 0.040250323712825775\n",
      "Epoch #191 iteration #250 loss: 0.04647131264209747\n",
      "Epoch #191 iteration #300 loss: 0.04671647027134895\n",
      "Took 1.785 minutes for epoch# 191 to train\n",
      "Epoch #191 Train loss: 0.044814085408758655\n",
      "Epoch #191 Validation Precision: 0.6874999999999999\n",
      "Epoch #192 iteration #50 loss: 0.052714839577674866\n",
      "Epoch #192 iteration #100 loss: 0.04649095609784126\n",
      "Epoch #192 iteration #150 loss: 0.040496595203876495\n",
      "Epoch #192 iteration #200 loss: 0.07381643354892731\n",
      "Epoch #192 iteration #250 loss: 0.04292917624115944\n",
      "Epoch #192 iteration #300 loss: 0.05233243107795715\n",
      "Took 1.782 minutes for epoch# 192 to train\n",
      "Epoch #192 Train loss: 0.04551679937718197\n",
      "Epoch #192 Validation Precision: 0.6666666666666666\n",
      "Epoch #193 iteration #50 loss: 0.04259930178523064\n",
      "Epoch #193 iteration #100 loss: 0.06154344603419304\n",
      "Epoch #193 iteration #150 loss: 0.05638996511697769\n",
      "Epoch #193 iteration #200 loss: 0.03396029397845268\n",
      "Epoch #193 iteration #250 loss: 0.0536496639251709\n",
      "Epoch #193 iteration #300 loss: 0.04225321114063263\n",
      "Took 1.780 minutes for epoch# 193 to train\n",
      "Epoch #193 Train loss: 0.0451581963003637\n",
      "Epoch #193 Validation Precision: 0.6666666666666666\n",
      "Epoch #194 iteration #50 loss: 0.06470435857772827\n",
      "Epoch #194 iteration #100 loss: 0.04220297560095787\n",
      "Epoch #194 iteration #150 loss: 0.029771899804472923\n",
      "Epoch #194 iteration #200 loss: 0.050397299230098724\n",
      "Epoch #194 iteration #250 loss: 0.04150960221886635\n",
      "Epoch #194 iteration #300 loss: 0.059888146817684174\n",
      "Took 1.782 minutes for epoch# 194 to train\n",
      "Epoch #194 Train loss: 0.04535866006373022\n",
      "Epoch #194 Validation Precision: 0.5416666666666666\n",
      "Epoch #195 iteration #50 loss: 0.05659041926264763\n",
      "Epoch #195 iteration #100 loss: 0.03488080948591232\n",
      "Epoch #195 iteration #150 loss: 0.041767336428165436\n",
      "Epoch #195 iteration #200 loss: 0.055126212537288666\n",
      "Epoch #195 iteration #250 loss: 0.03383885324001312\n",
      "Epoch #195 iteration #300 loss: 0.04756101220846176\n",
      "Took 1.786 minutes for epoch# 195 to train\n",
      "Epoch #195 Train loss: 0.04526563714115092\n",
      "Epoch #195 Validation Precision: 0.5\n",
      "Epoch #196 iteration #50 loss: 0.05348765850067139\n",
      "Epoch #196 iteration #100 loss: 0.03908507153391838\n",
      "Epoch #196 iteration #150 loss: 0.05531805008649826\n",
      "Epoch #196 iteration #200 loss: 0.049627646803855896\n",
      "Epoch #196 iteration #250 loss: 0.06103101000189781\n",
      "Epoch #196 iteration #300 loss: 0.05106557160615921\n",
      "Took 1.785 minutes for epoch# 196 to train\n",
      "Epoch #196 Train loss: 0.04396355502570096\n",
      "Epoch #196 Validation Precision: 0.37499999999999994\n",
      "Epoch #197 iteration #50 loss: 0.06505133211612701\n",
      "Epoch #197 iteration #100 loss: 0.06598930805921555\n",
      "Epoch #197 iteration #150 loss: 0.03151432424783707\n",
      "Epoch #197 iteration #200 loss: 0.029442433267831802\n",
      "Epoch #197 iteration #250 loss: 0.06047904118895531\n",
      "Epoch #197 iteration #300 loss: 0.04850875958800316\n",
      "Took 1.781 minutes for epoch# 197 to train\n",
      "Epoch #197 Train loss: 0.04445886197022329\n",
      "Epoch #197 Validation Precision: 0.6041666666666666\n",
      "Epoch #198 iteration #50 loss: 0.049699775874614716\n",
      "Epoch #198 iteration #100 loss: 0.027421213686466217\n",
      "Epoch #198 iteration #150 loss: 0.030160464346408844\n",
      "Epoch #198 iteration #200 loss: 0.04826042801141739\n",
      "Epoch #198 iteration #250 loss: 0.04551755264401436\n",
      "Epoch #198 iteration #300 loss: 0.042197033762931824\n",
      "Took 1.784 minutes for epoch# 198 to train\n",
      "Epoch #198 Train loss: 0.04303722786836517\n",
      "Epoch #198 Validation Precision: 0.6458333333333333\n",
      "Epoch #199 iteration #50 loss: 0.05019460991024971\n",
      "Epoch #199 iteration #100 loss: 0.03409043699502945\n",
      "Epoch #199 iteration #150 loss: 0.03430918604135513\n",
      "Epoch #199 iteration #200 loss: 0.038920629769563675\n",
      "Epoch #199 iteration #250 loss: 0.03717101737856865\n",
      "Epoch #199 iteration #300 loss: 0.04088533669710159\n",
      "Took 1.783 minutes for epoch# 199 to train\n",
      "Epoch #199 Train loss: 0.04321608461964971\n",
      "Epoch #199 Validation Precision: 0.5833333333333333\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n",
      "Figure(640x480)\n"
     ]
    }
   ],
   "source": [
    "!python train.py --show-sample yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T06:24:35.870532Z",
     "start_time": "2020-12-05T06:24:35.745348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec  5 14:24:35 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.59       Driver Version: 440.59       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| 55%   62C    P0    91W / 260W |    382MiB / 11016MiB |      8%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1280      G   /usr/lib/xorg/Xorg                           129MiB |\r\n",
      "|    0      1515      G   /usr/bin/gnome-shell                         172MiB |\r\n",
      "|    0      2159      G   ...AAAAAAAAAAAAAAgAAAAAAAAA --shared-files    77MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T06:24:35.891391Z",
     "start_time": "2020-12-05T06:24:35.878923Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import albumentations as A\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from PIL import Image\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "DIR_INPUT = '../input/rsna-pneumonia-detection-2018/input'\n",
    "DIR_TEST = f\"{DIR_INPUT}/samples\"\n",
    "test_images = os.listdir(DIR_TEST)\n",
    "print(f\"Validation instances: {len(test_images)}\")\n",
    "\n",
    "# load a model; pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, min_size=1024)\n",
    "num_classes = 2  # 1 class (pnueomonia) + background\n",
    "# get the number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "os.makedirs('../validation_predictions', exist_ok=True)\n",
    "model.load_state_dict(torch.load('../input/rsna-pytorch-hackathon-fasterrcnn-resnet-training/fasterrcnn_resnet50_fpn.pth'))\n",
    "model.to(device)\n",
    "\n",
    "def format_prediction_string(boxes, scores):\n",
    "    pred_strings = []\n",
    "    for j in zip(scores, boxes):\n",
    "        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], \n",
    "                                                             int(j[1][0]), int(j[1][1]), \n",
    "                                                             int(j[1][2]), int(j[1][3])))\n",
    "\n",
    "    return \" \".join(pred_strings)\n",
    "\n",
    "detection_threshold = 0.9\n",
    "img_num = 0\n",
    "results = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, image in tqdm(enumerate(test_images), total=len(test_images)):\n",
    "\n",
    "        orig_image = cv2.imread(f\"{DIR_TEST}/{test_images[i]}\", cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float)\n",
    "        image = torch.tensor(image, dtype=torch.float).cuda()\n",
    "        image = torch.unsqueeze(image, 0)\n",
    "\n",
    "        model.eval()\n",
    "        cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "        outputs = model(image)\n",
    "        \n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        if len(outputs[0]['boxes']) != 0:\n",
    "            for counter in range(len(outputs[0]['boxes'])):\n",
    "                boxes = outputs[0]['boxes'].data.cpu().numpy()\n",
    "                scores = outputs[0]['scores'].data.cpu().numpy()\n",
    "                boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "                draw_boxes = boxes.copy()\n",
    "                boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "                boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "                \n",
    "            for box in draw_boxes:\n",
    "                cv2.rectangle(orig_image,\n",
    "                            (int(box[0]), int(box[1])),\n",
    "                            (int(box[2]), int(box[3])),\n",
    "                            (0, 0, 255), 3)\n",
    "        \n",
    "            plt.imshow(cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB))\n",
    "            plt.axis('off')\n",
    "            plt.savefig(f\"{test_images[i]}\")\n",
    "            plt.close()\n",
    "                \n",
    "            result = {\n",
    "                'patientId': test_images[i].split('.')[0],\n",
    "                'PredictionString': format_prediction_string(boxes, scores)\n",
    "            }\n",
    "            results.append(result)\n",
    "        else:\n",
    "            result = {\n",
    "                'patientId': test_images[i].split('.')[0],\n",
    "                'PredictionString': None\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "sub_df = pd.DataFrame(results, columns=['patientId', 'PredictionString'])\n",
    "print(sub_df.head())\n",
    "sub_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
